{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historic-portfolio",
   "metadata": {},
   "source": [
    "### Babatunde Adewumi\n",
    "### Exercise 11.1-.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "southern-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy import matlib\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "constitutional-issue",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Getting a list of directory contents\n",
    "def gettextlist(directory_path):\n",
    "    directory_textfiles=[]\n",
    "    directory_nontextfiles=[]\n",
    "    directory_nonfiles=[]\n",
    "    # Process each item in the directory\n",
    "    directory_contents=os.listdir(directory_path)\n",
    "    for contentitem in directory_contents:\n",
    "        temp_fullpath=os.path.join(directory_path, contentitem)\n",
    "        # Non-files (e.g. subdirectories) are stored separately\n",
    "        if os.path.isfile(temp_fullpath)==0:\n",
    "            directory_nonfiles.append(contentitem)\n",
    "        else:\n",
    "            # Is this a non-text file (not ending in .txt)?\n",
    "            if temp_fullpath.find('.txt')==-1:\n",
    "                directory_nontextfiles.append(contentitem)\n",
    "            else:\n",
    "                # This is a text file\n",
    "                directory_textfiles.append(contentitem)\n",
    "    return(directory_textfiles,directory_nontextfiles,directory_nonfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ignored-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Basic file crawler\n",
    "def basicfilecrawler(directory_path):\n",
    "    # Store filenames read and their text content\n",
    "    num_files_read=0\n",
    "    crawled_filenames=[]\n",
    "    mycrawled_texts=[]\n",
    "    directory_contentlists=gettextlist(directory_path)\n",
    "    # In this basic crawled we just process text files\n",
    "    # and do not handle subdirectories\n",
    "    #directory_textfiles=directory_contentlists[0]\n",
    "    directory_nontextfiles=directory_contentlists[1]\n",
    "    for contentitem in directory_nontextfiles:\n",
    "        #print('Reading file:')\n",
    "        #print(contentitem)\n",
    "        # Open the file and read its contents\n",
    "        temp_fullpath=os.path.join(directory_path, contentitem)\n",
    "        #temp_file=open(temp_fullpath,'r',encoding='utf-8',errors='ignore')\n",
    "        temp_file=open(temp_fullpath,'r')\n",
    "        temp_text=temp_file.read()\n",
    "        temp_file.close()\n",
    "        # Store the read filename and content\n",
    "        crawled_filenames.append(contentitem)\n",
    "        mycrawled_texts.append(temp_text)\n",
    "        num_files_read=num_files_read+1\n",
    "    return(crawled_filenames, mycrawled_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "expired-rubber",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_filenames, mycrawled_texts = basicfilecrawler('20news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "industrial-bulletin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mycrawled_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "beginning-latin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100521', '101551', '101552', '101553', '101554']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_filenames[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-iraqi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude header lines from each message\n",
    "excludedlinemarkers=['Xref:','Path:','From:','Newsgroups:','Subject:','Summary:',\n",
    "                     'Keywords:','Message-ID:','Date:','Expires:','Followup-To:','Distribution:', \n",
    "                     'Organization:','Approved:','Supersedes:','Lines:','NNTP-Posting-Host:', \n",
    "                     'References:','Sender:','In-Reply-To:','Article-I.D.:','Reply-To:', \n",
    "                     'Nntp-Posting-Host:', 'Newsgroup', 'document_id']\n",
    "\n",
    "for k in range(len(mycrawled_texts)):\n",
    "    print(k)\n",
    "    templines = mycrawled_texts[k].splitlines()\n",
    "    remaininglines = []\n",
    "    for l in range(len(templines)):\n",
    "        line_should_be_excluded = 0\n",
    "        for m in range(len(excludedlinemarkers)):\n",
    "            if len(templines[l]) >= len(excludedlinemarkers[m]):\n",
    "                if excludedlinemarkers[m] == templines[l][0:len(excludedlinemarkers[m])]:\n",
    "                    line_should_be_excluded = 1\n",
    "                    break\n",
    "        if line_should_be_excluded == 0:\n",
    "            remaininglines.append(templines[l])\n",
    "    mycrawled_texts[k] = '\\n'.join(remaininglines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "sufficient-wednesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_texts = []\n",
    "# remove leading and trailing whitespaces\n",
    "for k in range(len(mycrawled_texts)):\n",
    "    temp_texts = mycrawled_texts[k].strip()\n",
    "    temp_texts = ' '.join(temp_texts.split())\n",
    "    downloaded_texts.append(temp_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "assigned-reading",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(downloaded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "relevant-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize downloaded texts and change them to NLTK format\n",
    "mydownloaded_nltk_texts = []\n",
    "for k in range(len(downloaded_texts)):\n",
    "    temp_tokenizedtext = nltk.word_tokenize(downloaded_texts[k])\n",
    "    temp_nltktext = nltk.Text(temp_tokenizedtext)\n",
    "    mydownloaded_nltk_texts.append(temp_nltktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "further-credit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all downloaded texts lowercase\n",
    "mydownloaded_lowercase_texts = []\n",
    "for k in range(len(mydownloaded_nltk_texts)):\n",
    "    temp_lowercase_text = []\n",
    "    for l in range(len(mydownloaded_nltk_texts[k])):\n",
    "        lowercase_word = mydownloaded_nltk_texts[k][l].lower()\n",
    "        temp_lowercase_text.append(lowercase_word)\n",
    "    temp_lowercasetest = nltk.Text(temp_lowercase_text)\n",
    "    mydownloaded_lowercase_texts.append(temp_lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "essential-kruger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydownloaded_lowercase_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ultimate-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a POS tag for WordNet\n",
    "def tagtowordnet(postag):\n",
    "    wordnettag=-1\n",
    "    if postag[0]=='N':\n",
    "        wordnettag='n'\n",
    "    elif postag[0]=='V':\n",
    "        wordnettag='v'\n",
    "    elif postag[0]=='J':\n",
    "        wordnettag='a'\n",
    "    elif postag[0]=='R':\n",
    "        wordnettag='r'\n",
    "    return(wordnettag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "distant-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# POS tag and lemmatize the loaded texts\n",
    "# Download tagger and wordnet resources if you do not have them already\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatize downloaded texts\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):\n",
    "    # tag the text with POS tags\n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)\n",
    "    # lemmatize each word text\n",
    "    lemmatizedtext = []\n",
    "    for l in range(len(taggedtext)):\n",
    "        # Lemmatize a word using the WordNet converted POS tag\n",
    "        wordtolemmatize = taggedtext[l][0]\n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])\n",
    "        if wordnettag != -1:\n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize, wordnettag)\n",
    "        else:\n",
    "            lemmatizedword = wordtolemmatize\n",
    "        # store the lemmatized word\n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "    return(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "virtual-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydownloaded_lemmatizedtexts = []\n",
    "for k in range(len(mydownloaded_lowercase_texts)):\n",
    "    lemmatizedtext = lemmatizetext(mydownloaded_lowercase_texts[k])\n",
    "    lemmatizedtext = nltk.Text(lemmatizedtext)\n",
    "    mydownloaded_lemmatizedtexts.append(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "removable-genetics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Text: the oriole ' pitch staff again be have...>,\n",
       " <Text: in article < c4vir5.l3r @ shuksan.ds.boeing.com > ,...>,\n",
       " <Text: in article < 1993apr5.135153.11132 @ wdl.loral.com > gwm...>,\n",
       " <Text: thanks to all of you who respond to...>,\n",
       " <Text: the subject say it all . my 1984...>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydownloaded_lemmatizedtexts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "outside-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the vocabularies of each document\n",
    "myvocabularies = []\n",
    "myindices_in_vocabularies = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # get unique words and where they occur\n",
    "    temptext = mydownloaded_lemmatizedtexts[k]\n",
    "    unique_results = np.unique(temptext, return_inverse = True)\n",
    "    unique_words = unique_results[0]\n",
    "    word_indices = unique_results[1]\n",
    "    # store the vocabularies and the indices\n",
    "    myvocabularies.append(unique_words)\n",
    "    myindices_in_vocabularies.append(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "neither-windsor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myvocabularies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "helpful-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the vocabularies\n",
    "# first concatenate all vocabularies\n",
    "tempvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "\n",
    "# find unique words among all the vocabularies\n",
    "uniqueresults = np.unique(tempvocabulary, return_inverse = True)\n",
    "unifiedvocabulary = uniqueresults[0]\n",
    "wordindices = uniqueresults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "third-wages",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42763"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "sonic-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate previous indices to the unified vocabulary\n",
    "# must keep track where each vocabulary started in the concatenated one\n",
    "vocabularystart = 0\n",
    "myindices_in_unifiedvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # in order to shift word indices, we must temporarily\n",
    "    # convert their data type to a numpy array\n",
    "    tempindices = np.array(myindices_in_vocabularies[k])\n",
    "    tempindices = tempindices + vocabularystart\n",
    "    tempindices = wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart = vocabularystart + len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "supposed-institute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42763"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "close-restaurant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myindices_in_unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "suitable-reminder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24498, 12907, 10996, 15189, 11089, 36258, 11087,   947, 21631,\n",
       "       11089], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myindices_in_unifiedvocabulary[1][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "classified-ceiling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary[myindices_in_unifiedvocabulary[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "prescription-woman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydownloaded_lemmatizedtexts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "false-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total count of each unique word over all the downloaded documents\n",
    "unifiedvocabulary_totaloccurrencecounts = np.zeros((len(unifiedvocabulary),1))\n",
    "\n",
    "# count occurrences\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    occurrencecounts = np.zeros((len(unifiedvocabulary), 1))\n",
    "    for l in range(len(myindices_in_unifiedvocabulary[k])):\n",
    "        occurrencecounts[myindices_in_unifiedvocabulary[k][l]] = (occurrencecounts[myindices_in_unifiedvocabulary[k][l]]\n",
    "                                                                 + 1)\n",
    "    unifiedvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts + occurrencecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "muslim-compatibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.079e+03],\n",
       "       [2.518e+03],\n",
       "       [1.224e+03],\n",
       "       ...,\n",
       "       [2.000e+00],\n",
       "       [6.000e+00],\n",
       "       [2.000e+00]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unifiedvocabulary_totaloccurrencecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "behind-williams",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42763, 42763)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary_totaloccurrencecounts), len(unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "identified-feedback",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>' 'the' ',' '.' '--' 'be' 'a' 'to' 'i' ')' 'in' 'and' '(' 'of' ':' '@'\n",
      " 'have' 'that' 'it' 'you' '!' 'do' 'for' '?' \"'s\" 'on' '|' \"n't\" '-'\n",
      " 'this' 'with' 'but' 'he' \"''\" 'not' '0' 'they' '...' '1' 'as' '<' '``'\n",
      " 'at' 'if' 'get' 'write' 'or' 'my' '2' 'article' 'game' 'go' 'would' '#'\n",
      " 'can' 'about' 'one' 'will' 'all' 'there' 'an' 'what' 'from' 'out' 'so'\n",
      " '*' 'good' 'car' 'like' 'year' 'by' 'think' 'me' 'team' 'up' 'his' '%'\n",
      " 'just' 'when' '3' 'make' 'more' 'no' 'your' 'know' 'any' 'say' 'who'\n",
      " 'than' 'we' '4' 'time' 'some' 'play' 'well' 'see' 'how' 'player' \"'m\"\n",
      " 'only']\n",
      "[40640. 40004. 39497. 39447. 35859. 27283. 17906. 17356. 16024. 14883.\n",
      " 14093. 14011. 13692. 13179. 11444.  9901.  9497.  9222.  8767.  7162.\n",
      "  7079.  7035.  7012.  6686.  6181.  6043.  5925.  5222.  4878.  4748.\n",
      "  4509.  4285.  4178.  4150.  4000.  3977.  3935.  3868.  3795.  3751.\n",
      "  3648.  3629.  3567.  3525.  3266.  3261.  3238.  3217.  2837.  2745.\n",
      "  2726.  2692.  2684.  2518.  2408.  2392.  2371.  2306.  2274.  2270.\n",
      "  2257.  2255.  2250.  2216.  2203.  2109.  2092.  2077.  2070.  2061.\n",
      "  2043.  2039.  1972.  1952.  1931.  1879.  1869.  1864.  1857.  1817.\n",
      "  1761.  1755.  1755.  1752.  1750.  1740.  1733.  1719.  1572.  1567.\n",
      "  1560.  1520.  1490.  1458.  1420.  1397.  1362.  1349.  1317.  1303.]\n"
     ]
    }
   ],
   "source": [
    "# top-100 words according to the largest total occurrence count\n",
    "highest_totaloccurrences_indices = np.argsort(-1*unifiedvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(unifiedvocabulary[highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(unifiedvocabulary_totaloccurrencecounts[highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "classical-birth",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Vocabulary pruning\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "pruningdecisions = np.zeros((len(unifiedvocabulary),1))\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    # Rule 1: check the nltk stop word list\n",
    "    if (unifiedvocabulary[k] in nltkstopwords):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 2: if the word is too short\n",
    "    if (len(unifiedvocabulary[k]) < 3):\n",
    "        pruningdecisions[k] = 1\n",
    "      # Rule 3: if the word is too long\n",
    "    if (len(unifiedvocabulary[k]) > 20):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 4: if the word is in the top 1% of frequent words\n",
    "    if (k in highest_totaloccurrences_indices[0:int(np.floor(len(unifiedvocabulary)*0.01))]):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 5: if the word occurs less than 4 times\n",
    "    if(unifiedvocabulary_totaloccurrencecounts[k] < 4):\n",
    "        pruningdecisions[k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "starting-submission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 words after pruning the unified vocabulary:\n",
      "\n",
      "['ago' 'cost' 'large' 'city' 'mind' 'total' 'mention' 'brave' 'man' 'ford'\n",
      " 'penalty' 'final' 'anyway' 'bill' 'coach' 'idea' 'saw' 'performance'\n",
      " 'beat' 'version' 'rule' 'montreal' 'hitter' 'seat' 'group' 'friend' 'ice'\n",
      " 'today' 'face' 'although' 'order' 'almost' 'comment' 'minute' 'stats'\n",
      " 'bos' 'hold' 'det' 'follow' 'job' 'american' 'insurance' 'month' 'smith'\n",
      " 'past' 'local' 'easy' 'honda' 'tie' 'cal' 'news' 'hell' 'x-newsreader'\n",
      " 'wait' 'ticket' 'bring' 'jet' 'walk' 'hope' 'helmet' 'rider' 'joe'\n",
      " 'stuff' 'morris' 'van' 'left' 'compare' 'note' 'word' 'york' 'experience'\n",
      " 'center' 'flyer' 'information' 'add' 'break' 'defense' 'puck' 'set' 'pen'\n",
      " 'rear' 'contact' 'netcom.com' 'others' 'e-mail' 'later' 'young' 'claim'\n",
      " 'late' 'whether' 'tor' 'design' 'matter' 'pit' 'life' 'pull' 'decide'\n",
      " 'instead' 'open' 'difference']\n",
      "[247. 246. 246. 246. 245. 244. 244. 243. 242. 241. 240. 240. 236. 232.\n",
      " 231. 230. 229. 229. 228. 225. 223. 223. 223. 222. 222. 222. 222. 221.\n",
      " 221. 220. 219. 219. 218. 217. 217. 217. 216. 215. 215. 213. 213. 213.\n",
      " 212. 211. 211. 211. 210. 210. 210. 209. 208. 208. 207. 206. 206. 206.\n",
      " 204. 204. 204. 204. 203. 203. 203. 202. 201. 201. 201. 200. 200. 200.\n",
      " 199. 198. 197. 197. 197. 196. 196. 196. 196. 196. 196. 196. 195. 195.\n",
      " 193. 193. 192. 192. 191. 189. 189. 189. 189. 187. 187. 187. 186. 186.\n",
      " 186. 185.]\n"
     ]
    }
   ],
   "source": [
    "print('Top-100 words after pruning the unified vocabulary:\\n')\n",
    "remaining_indices = np.squeeze(np.where(pruningdecisions==0)[0])\n",
    "remaining_vocabulary = unifiedvocabulary[remaining_indices]\n",
    "remainingvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts[remaining_indices]\n",
    "remaining_highest_totaloccurrences_indices = np.argsort(-1*remainingvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(remaining_vocabulary[remaining_highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(remainingvocabulary_totaloccurrencecounts[remaining_highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "leading-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get indices of documents to remaining words\n",
    "oldtopruned=[]\n",
    "tempind=-1\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    if pruningdecisions[k]==0:\n",
    "        tempind=tempind+1\n",
    "        oldtopruned.append(tempind)\n",
    "    else:\n",
    "        oldtopruned.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "selective-amazon",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create pruned texts\n",
    "mycrawled_prunedtexts=[]\n",
    "myindices_in_prunedvocabulary=[]\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    #print(k)\n",
    "    temp_newindices=[]\n",
    "    temp_newdoc=[]\n",
    "    for l in range(len(mydownloaded_lemmatizedtexts[k])):\n",
    "        temp_oldindex=myindices_in_unifiedvocabulary[k][l]\n",
    "        temp_newindex=oldtopruned[temp_oldindex]\n",
    "        if temp_newindex!=-1:\n",
    "            temp_newindices.append(temp_newindex)\n",
    "            temp_newdoc.append(unifiedvocabulary[temp_oldindex])\n",
    "    mycrawled_prunedtexts.append(temp_newdoc)\n",
    "    myindices_in_prunedvocabulary.append(temp_newindices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "dietary-oxide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mycrawled_prunedtexts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "certain-dragon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12087"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "wicked-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create count vectors\n",
    "n_docs = len(mycrawled_prunedtexts) # 4000=total number of documents\n",
    "n_vocab = len(remaining_vocabulary) # 12087\n",
    "# matrix of term frequencies\n",
    "tfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab)) # 4000x12087\n",
    "# row vector of document frequencies\n",
    "dfvector = scipy.sparse.lil_matrix((1, n_vocab)) #1x12087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "first-possession",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 12087)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_docs, n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "revised-surgeon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4000, 12087), (1, 12087))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrix.shape, dfvector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "guided-staff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the documents\n",
    "for k in range(n_docs):\n",
    "    # row vector of which words occur in this document\n",
    "    temp_dfvector = scipy.sparse.lil_matrix((1, n_vocab)) # #1x12087\n",
    "    # we loop over the words in each document\n",
    "    for l in range(len(mycrawled_prunedtexts[k])):\n",
    "        # add current word to term-frequency count and document count\n",
    "        currentword = myindices_in_prunedvocabulary[k][l]\n",
    "        tfmatrix[k, currentword] = tfmatrix[k, currentword] + 1\n",
    "        #tfmatrix[k, currentword] = tfmatrix[k, currentword]/len(mycrawled_prunedtexts[k])\n",
    "        temp_dfvector[0, currentword] = 1\n",
    "    # add which words occur in this document to overall document counts\n",
    "    \n",
    "    dfvector = dfvector + temp_dfvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "nasty-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length-normalized frequency for the TF part\n",
    "for i in range(n_docs):\n",
    "    for j in range(len(tfmatrix.data[i])):\n",
    "        tfmatrix.data[i][j] = tfmatrix.data[i][j] / len(mycrawled_prunedtexts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "measured-kelly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0000000000000009, 0.9999999999999999)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tfmatrix.data[0]), sum(tfmatrix.data[1]) # sum to 1 because of the normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "headed-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9999999999999994, 1.0000000000000002)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tfmatrix.data[2]), sum(tfmatrix.data[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "annual-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create IDF and TF-IDF vectors\n",
    "# smoothed logarithmic idf\n",
    "idfvector = np.squeeze(np.array(dfvector.todense()))\n",
    "idfvector = 1 + np.log(((idfvector + 1) ** -1) * n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "anonymous-drinking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.89615437, 7.68461173, 6.89615437, ..., 7.50229017, 6.99146455,\n",
       "       7.34813949])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "instructional-alias",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the count statistics to compute the tf-idf matrix\n",
    "tfidfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab))\n",
    "# use the count statistics to compute the tf-idf matrix\n",
    "for k in range(n_docs):\n",
    "    # find nonzero term frequencies\n",
    "    tempindices = np.nonzero(tfmatrix[k, :])[1]\n",
    "    tfterm = np.squeeze(np.array(tfmatrix[k, tempindices].todense()))\n",
    "    tfidfmatrix[k, tempindices] = tfterm * idfvector[tempindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "piano-panic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 12087)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfmatrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-australia",
   "metadata": {},
   "source": [
    "#### Exercise 11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "streaming-visibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# , files_directory, = gettextlist('20news')\n",
    "directory_indices = []\n",
    "for i in range(len(mycrawled_texts)):\n",
    "    directory_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "divided-preparation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(directory_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "driving-negotiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2701984c370>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidfmatrix.copy()\n",
    "# Normalize tf-idf vector norms\n",
    "for k in range(n_docs):\n",
    "    # print(k)\n",
    "    X[k,:] = X[k,:] / np.sqrt(np.sum(X[k,:].multiply(X[k,:]), axis = 1)[0] + 0.0000000001)\n",
    "# Plot projected documents\n",
    "svdmodel = sklearn.decomposition.TruncatedSVD(n_components = 2, n_iter = 70, random_state = 42)\n",
    "documentplot = svdmodel.fit(X).transform(X)\n",
    "%matplotlib auto\n",
    "myfigure, myaxes = plt.subplots()\n",
    "myaxes.scatter(documentplot[:, 0], documentplot[:, 1], c=directory_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "allied-edwards",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentplot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "least-opera",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00109724, 0.00378467])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdmodel.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "quality-pregnancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0048819068749381255"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdmodel.explained_variance_ratio_.sum() # account for only 5% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "sensitive-command",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0025952 ,  0.00113445,  0.0025952 , ...,  0.00210671,\n",
       "         0.0030956 ,  0.00196754],\n",
       "       [-0.00398187, -0.00195219, -0.00398187, ..., -0.00288292,\n",
       "        -0.00597959, -0.00091849]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svdmodel.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-shirt",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "- **The scatter plot seems to show that there are four categories of newsgroups as indicated by the colors.**\n",
    "- **The PCA plot shows that the document features(vocabulary) are distributed along the 2 principal components axes.**\n",
    "- **The 2 prinpal components only account for about 5% variance, and so most of the features are lost.**\n",
    "- **The four categories of the newsgroups are not well separated. This is because the total variance accounted for by the 2 principal components is very small.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-begin",
   "metadata": {},
   "source": [
    "#### Exercise 11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "documentary-score",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 61 nearest neighbors...\n",
      "[t-SNE] Indexed 4000 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 4000 samples in 1.024s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 4000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 4000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 4000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 4000\n",
      "[t-SNE] Mean sigma: 0.086499\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 88.414658\n",
      "[t-SNE] KL divergence after 400 iterations: 1.642098\n",
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "#%% PCA followed by t-SNE\n",
    "# Find a 100-dimensional PCA projection\n",
    "svdmodel=sklearn.decomposition.TruncatedSVD(n_components=100, n_iter=70, random_state=42)\n",
    "pcaplot = svdmodel.fit(X).transform(X)\n",
    "\n",
    "# Run t-SNE\n",
    "tsnemodel = sklearn.manifold.TSNE(n_components=2, verbose=1, perplexity=20, n_iter=400)\n",
    "tsneplot = tsnemodel.fit_transform(pcaplot)\n",
    "# Plot the result\n",
    "%matplotlib auto\n",
    "myfigure, myaxes = plt.subplots();\n",
    "myaxes.scatter(tsneplot[:, 0], tsneplot[:, 1], c=directory_indices);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "adjacent-worthy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 2)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsneplot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valued-plenty",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "- **The categories of the newsgroups seem to be better separated to form clusters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-evidence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
