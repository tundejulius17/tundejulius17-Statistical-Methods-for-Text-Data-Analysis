{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-positive",
   "metadata": {},
   "source": [
    "### Exercise 4 - Document clustering, and introduction to N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-baptist",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "catholic-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy import matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "concrete-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basicfilecrawler(file_path):\n",
    "    # Store filenames read and their text content\n",
    "    #num_files_read=0\n",
    "    #crawled_filenames=[]\n",
    "    #crawled_texts=[]\n",
    "    #directory_contentlists=gettextlist(directory_path)\n",
    "    # In this basic crawled we just process text files\n",
    "    # and do not handle subdirectories\n",
    "    #directory_textfiles=directory_contentlists[0]\n",
    "    \n",
    "    temp_file=open(file_path,'r',encoding='utf-8',errors='ignore')\n",
    "    temp_text=temp_file.read() \n",
    "    mytext_paragraphs=re.split(r'\\n[ \\n]*\\n', temp_text) \n",
    "    temp_file.close()\n",
    "    \n",
    "    return(mytext_paragraphs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "assisted-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext_paragraphs = basicfilecrawler(\"the _wonderful _wizard.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "numerical-maintenance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mytext_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "front-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize downloaded texts paragraph by paragraph\n",
    "# and change them to NLTK format\n",
    "mydownloaded_nltk_texts = []\n",
    "for k in range(len(mytext_paragraphs)):\n",
    "    temp_tokenizedtext = nltk.word_tokenize(mytext_paragraphs[k])\n",
    "    temp_nltktext = nltk.Text(temp_tokenizedtext)\n",
    "    mydownloaded_nltk_texts.append(temp_nltktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "magnetic-indian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: Dorothy lived in the midst of the great...>"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydownloaded_nltk_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "rough-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all downloaded texts lowercase\n",
    "mydownloaded_lowercase_texts = []\n",
    "for k in range(len(mydownloaded_nltk_texts)):\n",
    "    temp_lowercase_text = []\n",
    "    for l in range(len(mydownloaded_nltk_texts[k])):\n",
    "        lowercase_word = mydownloaded_nltk_texts[k][l].lower()\n",
    "        temp_lowercase_text.append(lowercase_word)\n",
    "    temp_lowercasetest = nltk.Text(temp_lowercase_text)\n",
    "    mydownloaded_lowercase_texts.append(temp_lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "willing-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a POS tag for WordNet\n",
    "def tagtowordnet(postag):\n",
    "    wordnettag=-1\n",
    "    if postag[0]=='N':\n",
    "        wordnettag='n'\n",
    "    elif postag[0]=='V':\n",
    "        wordnettag='v'\n",
    "    elif postag[0]=='J':\n",
    "        wordnettag='a'\n",
    "    elif postag[0]=='R':\n",
    "        wordnettag='r'\n",
    "    return(wordnettag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "hired-diversity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# POS tag and lemmatize the loaded texts\n",
    "# Download tagger and wordnet resources if you do not have them already\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatize downloaded texts\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):\n",
    "    # tag the text with POS tags\n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)\n",
    "    # lemmatize each word text\n",
    "    lemmatizedtext = []\n",
    "    for l in range(len(taggedtext)):\n",
    "        # Lemmatize a word using the WordNet converted POS tag\n",
    "        wordtolemmatize = taggedtext[l][0]\n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])\n",
    "        if wordnettag != -1:\n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize, wordnettag)\n",
    "        else:\n",
    "            lemmatizedword = wordtolemmatize\n",
    "        # store the lemmatized word\n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "    return(lemmatizedtext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "roman-electric",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydownloaded_lemmatizedtexts = []\n",
    "for k in range(len(mydownloaded_lowercase_texts)):\n",
    "    lemmatizedtext = lemmatizetext(mydownloaded_lowercase_texts[k])\n",
    "    lemmatizedtext = nltk.Text(lemmatizedtext)\n",
    "    mydownloaded_lemmatizedtexts.append(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "technological-hunter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: `` from the land of oz , ''...>"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydownloaded_lemmatizedtexts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "binary-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the vocabularies of each document/paragraph\n",
    "myvocabularies = []\n",
    "myindices_in_vocabularies = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # get unique words and where they occur\n",
    "    temptext = mydownloaded_lemmatizedtexts[k]\n",
    "    unique_results = np.unique(temptext, return_inverse = True)\n",
    "    unique_words = unique_results[0]\n",
    "    word_indices = unique_results[1]\n",
    "    # store the vocabularies and the indices\n",
    "    myvocabularies.append(unique_words)\n",
    "    myindices_in_vocabularies.append(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "sustained-editing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myvocabularies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "matched-warrant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myindices_in_vocabularies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "steady-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the vocabularies\n",
    "# first concatenate all vocabularies\n",
    "tempvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "\n",
    "# find unique words among all the vocabularies\n",
    "uniqueresults = np.unique(tempvocabulary, return_inverse = True)\n",
    "unifiedvocabulary = uniqueresults[0]\n",
    "wordindices = uniqueresults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "published-merchant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2265"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "different-advancement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  14,   23,   24, ..., 2015, 2024, 2028], dtype=int64)"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "united-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate previous indices to the unified vocabulary\n",
    "# must keep track where each vocabulary started in the concatenated one\n",
    "vocabularystart = 0\n",
    "myindices_in_unifiedvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # in order to shift word indices, we must temporarily\n",
    "    # convert their data type to a numpy array\n",
    "    tempindices = np.array(myindices_in_vocabularies[k])\n",
    "    tempindices = tempindices + vocabularystart\n",
    "    tempindices = wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart = vocabularystart + len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "facial-assessment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1155"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myindices_in_unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "explicit-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total count of each unique word over all the downloaded documents\n",
    "unifiedvocabulary_totaloccurrencecounts=np.zeros((len(unifiedvocabulary),1))\n",
    "\n",
    "# count occurrences\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    occurrencecounts = np.zeros((len(unifiedvocabulary), 1))\n",
    "    for l in range(len(myindices_in_unifiedvocabulary[k])):\n",
    "        occurrencecounts[myindices_in_unifiedvocabulary[k][l]] = (occurrencecounts[myindices_in_unifiedvocabulary[k][l]]\n",
    "                                                                 + 1)\n",
    "    unifiedvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts + occurrencecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "further-capital",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2265"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary_totaloccurrencecounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "empty-contract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the' ',' '.' 'and' 'be' 'to' \"''\" '``' 'of' 'a' 'i' 'you' 'in' 'have'\n",
      " 'he' 'it' 'her' 'they' 'she' 'that' 'dorothy' 'say' 'as' 'for' ';' 'so'\n",
      " 'but' 'do' 'not' 'with' 'at' 'all' 'them' 'scarecrow' 'his' 'me' '?' 'my'\n",
      " 'him' 'come' 'woodman' 'lion' 'when' 'will' 'on' 'go' 'oz' 'we' 'make'\n",
      " 'great' 'this' 'ask' 'tin' 'witch' 'if' 'little' 'there' 'one' 'then'\n",
      " 'from' 'could' 'see' 'who' 'would' 'no' 'out' 'get' 'up' 'green' 'can'\n",
      " 'head' 'look' 'their' 'back' '!' 'down' 'girl' 'know' 'toto' 'over'\n",
      " 'think' 'by' 'what' 'answer' 'again' 'upon' 'good' 'very' 'where' 'give'\n",
      " 'shall' 'now' \"n't\" 'find' 'city' 'into' \"'s\" 'must' 'man' 'walk']\n",
      "[2934. 2719. 1850. 1668. 1476. 1109. 1086. 1079.  826.  802.  649.  496.\n",
      "  480.  479.  456.  427.  411.  402.  400.  394.  360.  359.  329.  323.\n",
      "  321.  308.  302.  283.  274.  272.  254.  246.  238.  226.  216.  198.\n",
      "  194.  189.  184.  182.  181.  180.  160.  159.  158.  156.  155.  150.\n",
      "  149.  147.  147.  139.  138.  138.  137.  136.  129.  128.  125.  124.\n",
      "  122.  122.  114.  113.  112.  108.  107.  107.  105.  103.  102.  101.\n",
      "  100.  100.   98.   96.   96.   96.   93.   91.   90.   89.   88.   87.\n",
      "   87.   85.   85.   85.   83.   83.   82.   82.   82.   81.   80.   80.\n",
      "   77.   75.   75.   72.]\n"
     ]
    }
   ],
   "source": [
    "# top-100 words according to the largest total occurrence count\n",
    "highest_totaloccurrences_indices = np.argsort(-1*unifiedvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(unifiedvocabulary[highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(unifiedvocabulary_totaloccurrencecounts[highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "bibliographic-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Vocabulary pruning\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "pruningdecisions = np.zeros((len(unifiedvocabulary),1))\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    # Rule 1: check the nltk stop word list\n",
    "    if (unifiedvocabulary[k] in nltkstopwords):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 2: if the word is too short\n",
    "    if (len(unifiedvocabulary[k]) < 2):\n",
    "        pruningdecisions[k] = 1\n",
    "      # Rule 3: if the word is too long\n",
    "    if (len(unifiedvocabulary[k]) > 20):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 4: if the word occurs less than 4 times\n",
    "    if(unifiedvocabulary_totaloccurrencecounts[k] < 4):\n",
    "        pruningdecisions[k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "recognized-victim",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 words after pruning the unified vocabulary:\n",
      "\n",
      "[\"''\" '``' 'dorothy' 'say' 'scarecrow' 'come' 'woodman' 'lion' 'go' 'oz'\n",
      " 'make' 'great' 'ask' 'witch' 'tin' 'little' 'one' 'could' 'see' 'would'\n",
      " 'get' 'green' 'head' 'look' 'back' 'know' 'girl' 'toto' 'think' 'answer'\n",
      " 'upon' 'good' 'give' 'shall' \"n't\" 'find' 'city' \"'s\" 'man' 'must'\n",
      " 'wicked' 'walk' 'emerald' 'long' 'well' 'heart' 'take' 'room' 'away' 'us'\n",
      " 'country' 'like' 'way' 'big' 'time' 'carry' 'tree' 'people' 'tell' 'saw'\n",
      " 'brain' 'never' 'eye' 'reply' 'monkey' 'stand' 'many' 'day' 'live'\n",
      " 'first' 'run' 'help' 'road' 'ever' 'forest' 'friend' 'soon' 'house'\n",
      " 'keep' 'much' 'beast' 'wish' 'arm' 'around' 'sit' 'cry' 'wizard' 'thing'\n",
      " 'call' 'beautiful' 'old' 'oh' 'land' 'fly' 'shoe' 'woman' 'air' 'kill'\n",
      " 'kansas' 'quite']\n",
      "[1086. 1079.  360.  359.  226.  182.  181.  180.  156.  155.  149.  147.\n",
      "  139.  138.  138.  136.  128.  122.  122.  113.  107.  105.  102.  101.\n",
      "  100.   96.   96.   93.   90.   87.   85.   85.   83.   82.   82.   81.\n",
      "   80.   77.   75.   75.   72.   72.   71.   70.   69.   69.   68.   67.\n",
      "   67.   66.   66.   65.   65.   64.   63.   62.   62.   60.   60.   58.\n",
      "   58.   58.   57.   55.   52.   52.   50.   50.   50.   49.   48.   47.\n",
      "   46.   46.   45.   45.   44.   44.   43.   43.   42.   42.   42.   42.\n",
      "   42.   41.   40.   40.   39.   39.   38.   38.   38.   37.   37.   36.\n",
      "   36.   36.   36.   35.]\n"
     ]
    }
   ],
   "source": [
    "print('Top-100 words after pruning the unified vocabulary:\\n')\n",
    "remaining_indices = np.squeeze(np.where(pruningdecisions==0)[0])\n",
    "remaining_vocabulary = unifiedvocabulary[remaining_indices]\n",
    "remainingvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts[remaining_indices]\n",
    "remaining_highest_totaloccurrences_indices = np.argsort(-1*remainingvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(remaining_vocabulary[remaining_highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(remainingvocabulary_totaloccurrencecounts[remaining_highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "dutch-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get indices of documents to remaining words\n",
    "oldtopruned=[]\n",
    "tempind=-1\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    if pruningdecisions[k]==0:\n",
    "        tempind=tempind+1\n",
    "        oldtopruned.append(tempind)\n",
    "    else:\n",
    "        oldtopruned.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "israeli-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create pruned texts\n",
    "mycrawled_prunedtexts=[]\n",
    "myindices_in_prunedvocabulary=[]\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    #print(k)\n",
    "    temp_newindices=[]\n",
    "    temp_newdoc=[]\n",
    "    for l in range(len(mydownloaded_lemmatizedtexts[k])):\n",
    "        temp_oldindex=myindices_in_unifiedvocabulary[k][l]\n",
    "        temp_newindex=oldtopruned[temp_oldindex]\n",
    "        if temp_newindex!=-1:\n",
    "            temp_newindices.append(temp_newindex)\n",
    "            temp_newdoc.append(unifiedvocabulary[temp_oldindex])\n",
    "    mycrawled_prunedtexts.append(temp_newdoc)\n",
    "    myindices_in_prunedvocabulary.append(temp_newindices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "classified-second",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "802"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remaining_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "exempt-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create count vectors\n",
    "n_docs = len(mycrawled_prunedtexts) # 1449=total number of documents\n",
    "n_vocab = len(remaining_vocabulary) # 920\n",
    "# matrix of term frequencies\n",
    "tfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab)) # 1449x920\n",
    "# row vector of document frequencies\n",
    "dfvector = scipy.sparse.lil_matrix((1, n_vocab)) #1x920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "worthy-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mycrawled_prunedtexts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "integrated-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the documents\n",
    "for k in range(n_docs):\n",
    "    # row vector of which words occur in this document\n",
    "    temp_dfvector = scipy.sparse.lil_matrix((1, n_vocab)) # 1x920\n",
    "    # we loop over the words in each document\n",
    "    for l in range(len(mycrawled_prunedtexts[k])):\n",
    "        # add current word to term-frequency count and document count\n",
    "        currentword = myindices_in_prunedvocabulary[k][l]\n",
    "        tfmatrix[k, currentword] = tfmatrix[k, currentword] + 1\n",
    "        #tfmatrix[k, currentword] = tfmatrix[k, currentword]/len(mycrawled_prunedtexts[k])\n",
    "        temp_dfvector[0, currentword] = 1\n",
    "    # add which words occur in this document to overall document counts\n",
    "    \n",
    "    dfvector = dfvector + temp_dfvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "synthetic-prague",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1.0, 1.0, 1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 1.0, 1.0, 2.0, 2.0, 2.0, 1.0, 2.0, 1.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0]),\n",
       "       list([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 4.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 3.0, 1.0, 1.0, 1.0, 1.0]),\n",
       "       list([1.0, 1.0, 1.0, 2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
       "       ..., list([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
       "       list([2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]),\n",
       "       list([2.0, 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfmatrix.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "radical-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length-normalized frequency for the TF part\n",
    "for i in range(n_docs):\n",
    "    for j in range(len(tfmatrix.data[i])):\n",
    "        tfmatrix.data[i][j] = tfmatrix.data[i][j]/len(tfmatrix.data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "sorted-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create IDF and TF-IDF vectors\n",
    "# smoothed logarithmic idf\n",
    "idfvector = np.squeeze(np.array(dfvector.todense()))\n",
    "idfvector = np.log(1 + ((idfvector+1)**-1)*n_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "chronic-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the count statistics to compute the tf-idf matrix\n",
    "tfidfmatrix = scipy.sparse.lil_matrix((n_docs, n_vocab))\n",
    "# use the count statistics to compute the tf-idf matrix\n",
    "for k in range(n_docs):\n",
    "    # find nonzero term frequencies\n",
    "    tempindices = np.nonzero(tfmatrix[k, :])[1]\n",
    "    tfterm = np.squeeze(np.array(tfmatrix[k, tempindices].todense()))\n",
    "    tfidfmatrix[k, tempindices] = tfterm * idfvector[tempindices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "fixed-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Use the TF-IDF matrix as data to be clustered\n",
    "X=tfidfmatrix\n",
    "# Normalize the documents to unit vector norm\n",
    "tempnorms=np.squeeze(np.array(np.sum(X.multiply(X),axis=1)))\n",
    "# If any documents have zero norm, avoid dividing them by zero\n",
    "tempnorms[tempnorms==0]=1\n",
    "X=scipy.sparse.diags(tempnorms**-0.5).dot(X)\n",
    "n_data=np.shape(X)[0]\n",
    "n_dimensions=np.shape(X)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "satellite-indication",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Initialize the Gaussian mixture model\n",
    "# Function to initialize the Gaussian mixture model, create component parameters\n",
    "def initialize_mixturemodel(X,n_components):\n",
    "    # Create lists of sparse matrices to hold the parameters\n",
    "    n_dimensions=np.shape(X)[1]\n",
    "    n_data = np.shape(X)[0]\n",
    "    mixturemodel_means=scipy.sparse.lil_matrix((n_components,n_dimensions))\n",
    "    mixturemodel_weights=np.zeros((n_components))\n",
    "    mixturemodel_covariances=[]\n",
    "    mixturemodel_inversecovariances=[]\n",
    "    for k in range(n_components):\n",
    "        tempcovariance=scipy.sparse.lil_matrix((n_dimensions,n_dimensions))\n",
    "        mixturemodel_covariances.append(tempcovariance)\n",
    "        tempinvcovariance=scipy.sparse.lil_matrix((n_dimensions,n_dimensions))\n",
    "        mixturemodel_inversecovariances.append(tempinvcovariance)\n",
    "    # Initialize the parameters\n",
    "    for k in range(n_components):\n",
    "        mixturemodel_weights[k]=1/n_components\n",
    "        # Pick a random data point as the initial mean\n",
    "        tempindex=scipy.stats.randint.rvs(low=0,high=n_data)\n",
    "        mixturemodel_means[k]=X[tempindex,:].toarray()\n",
    "        # Initialize the covariance matrix to be spherical\n",
    "        for l in range(n_dimensions):\n",
    "            mixturemodel_covariances[k][l,l]=1\n",
    "            mixturemodel_inversecovariances[k][l,l]=1\n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "prescription-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_estep(X,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances,mixturemodel_weights):\n",
    "    # For each component, compute terms that do not involve data\n",
    "    meanterms=np.zeros((n_components))\n",
    "    logdeterminants=np.zeros((n_components))\n",
    "    logconstantterms=np.zeros((n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        # Compute mu_k*inv(Sigma_k)*mu_k\n",
    "        meanterms[k]=(mixturemodel_means[k,:]*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T)[0,0]\n",
    "        # Compute determinant of Sigma_k. For a diagonal matrix\n",
    "        # this is just the product of the main diagonal\n",
    "        logdeterminants[k]=np.sum(np.log(mixturemodel_covariances[k].diagonal(0)))\n",
    "        # Compute constant term beta_k * 1/(|Sigma_k|^1/2)\n",
    "        # Omit the (2pi)^d/2 as it cancels out\n",
    "        logconstantterms[k]=np.log(mixturemodel_weights[k]) - 0.5*logdeterminants[k]\n",
    "    \n",
    "    print('E-step part2 ')\n",
    "    # Compute terms that involve distances of data from components\n",
    "    xnorms=np.zeros((n_data,n_components))\n",
    "    xtimesmu=np.zeros((n_data,n_components))\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        #print(k)\n",
    "        xnorms[:,k]=(X*mixturemodel_inversecovariances[k]*X.T).diagonal(0)\n",
    "        xtimesmu[:,k]=np.squeeze((X*mixturemodel_inversecovariances[k]* mixturemodel_means[k,:].T).toarray())\n",
    "        \n",
    "    xdists=xnorms+np.matlib.repmat(meanterms,n_data,1)-2*xtimesmu\n",
    "    # Substract maximal term before exponent (cancels out) to maintain computational precision\n",
    "    numeratorterms=logconstantterms-xdists/2\n",
    "    numeratorterms-=np.matlib.repmat(np.max(numeratorterms,axis=1),n_components,1).T\n",
    "    numeratorterms=np.exp(numeratorterms)\n",
    "    mixturemodel_componentmemberships=numeratorterms/np.matlib.repmat(np.sum(numeratorterms,axis=1),n_components,1).T\n",
    "    return(mixturemodel_componentmemberships)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "educated-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mstep_sumweights(mixturemodel_componentmemberships):\n",
    "    # Compute total weight per component\n",
    "    mixturemodel_weights=np.sum(mixturemodel_componentmemberships,axis=0)\n",
    "    return(mixturemodel_weights)\n",
    "\n",
    "def run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights):\n",
    "    # Update component means\n",
    "    mixturemodel_means=scipy.sparse.lil_matrix((n_components,n_dimensions))\n",
    "    for k in range(n_components):\n",
    "        mixturemodel_means[k,:]=np.sum(scipy.sparse.diags(mixturemodel_componentmemberships[:,k]).dot(X),axis=0)\n",
    "        mixturemodel_means[k,:]/=mixturemodel_weights[k]\n",
    "    return(mixturemodel_means)\n",
    "\n",
    "def run_mstep_covariances(X,mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means):\n",
    "    # Update diagonal component covariance matrices\n",
    "    n_dimensions=np.shape(X)[1]\n",
    "    n_components=np.shape(mixturemodel_componentmemberships)[1]\n",
    "    tempcovariances=np.zeros((n_components,n_dimensions))\n",
    "    mixturemodel_covariances=[]\n",
    "    mixturemodel_inversecovariances=[]\n",
    "    \n",
    "    for k in range(n_components):\n",
    "        tempcovariances[k,:]= np.sum(scipy.sparse.diags(\n",
    "            mixturemodel_componentmemberships[:,k]).dot(\n",
    "            X.multiply(X)),axis=0)-mixturemodel_means[k,:].multiply(mixturemodel_means[k,:])*mixturemodel_weights[k]\n",
    "        tempcovariances[k,:]/=mixturemodel_weights[k]\n",
    "        # Convert to sparse matrices\n",
    "        tempepsilon=1e-10\n",
    "        # Add a small regularization term\n",
    "        temp_covariance=scipy.sparse.diags(tempcovariances[k,:]+tempepsilon)\n",
    "        temp_inversecovariance=scipy.sparse.diags((tempcovariances[k,:]+tempepsilon)**-1)\n",
    "        mixturemodel_covariances.append(temp_covariance)\n",
    "        mixturemodel_inversecovariances.append(temp_inversecovariance)\n",
    "    return(mixturemodel_covariances,mixturemodel_inversecovariances)\n",
    "\n",
    "def run_mstep_normalizeweights(mixturemodel_weights):\n",
    "    # Update mixture-component prior probabilities\n",
    "    mixturemodel_weights/=sum(mixturemodel_weights)\n",
    "    return(mixturemodel_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "impressed-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Perform the EM algorithm iterations\n",
    "def perform_emalgorithm(X,n_components,n_emiterations):\n",
    "    mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,\\\n",
    " mixturemodel_inversecovariances=initialize_mixturemodel(X,n_components)\n",
    "\n",
    "    for t in range(n_emiterations):\n",
    "        # ====== E-step: Compute the component membership\n",
    "        # probabilities of each data point ======\n",
    "        print('E-step ' + str(t))\n",
    "\n",
    "        mixturemodel_componentmemberships=run_estep(X,mixturemodel_means,mixturemodel_covariances,\n",
    "                                            mixturemodel_inversecovariances,mixturemodel_weights)\n",
    "        # ====== M-step: update component parameters======\n",
    "        print('M-step ' + str(t))\n",
    "        print('M-step part1 ' + str(t))\n",
    "        mixturemodel_weights=run_mstep_sumweights(mixturemodel_componentmemberships)\n",
    "        print('M-step part2 ' + str(t))\n",
    "\n",
    "        mixturemodel_means=run_mstep_means(X,mixturemodel_componentmemberships,mixturemodel_weights)\n",
    "        print('M-step part3 ' + str(t))\n",
    "        mixturemodel_covariances,mixturemodel_inversecovariances=run_mstep_covariances(\n",
    "            X,mixturemodel_componentmemberships,mixturemodel_weights,mixturemodel_means)\n",
    "        \n",
    "        print('M-step part4 ' + str(t))\n",
    "        mixturemodel_weights=run_mstep_normalizeweights(mixturemodel_weights)\n",
    "    return(mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "moved-briefs",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-step 0\n",
      "E-step part2 \n",
      "M-step 0\n",
      "M-step part1 0\n",
      "M-step part2 0\n",
      "M-step part3 0\n",
      "M-step part4 0\n",
      "E-step 1\n",
      "E-step part2 \n",
      "M-step 1\n",
      "M-step part1 1\n",
      "M-step part2 1\n",
      "M-step part3 1\n",
      "M-step part4 1\n",
      "E-step 2\n",
      "E-step part2 \n",
      "M-step 2\n",
      "M-step part1 2\n",
      "M-step part2 2\n",
      "M-step part3 2\n",
      "M-step part4 2\n",
      "E-step 3\n",
      "E-step part2 \n",
      "M-step 3\n",
      "M-step part1 3\n",
      "M-step part2 3\n",
      "M-step part3 3\n",
      "M-step part4 3\n",
      "E-step 4\n",
      "E-step part2 \n",
      "M-step 4\n",
      "M-step part1 4\n",
      "M-step part2 4\n",
      "M-step part3 4\n",
      "M-step part4 4\n",
      "E-step 5\n",
      "E-step part2 \n",
      "M-step 5\n",
      "M-step part1 5\n",
      "M-step part2 5\n",
      "M-step part3 5\n",
      "M-step part4 5\n",
      "E-step 6\n",
      "E-step part2 \n",
      "M-step 6\n",
      "M-step part1 6\n",
      "M-step part2 6\n",
      "M-step part3 6\n",
      "M-step part4 6\n",
      "E-step 7\n",
      "E-step part2 \n",
      "M-step 7\n",
      "M-step part1 7\n",
      "M-step part2 7\n",
      "M-step part3 7\n",
      "M-step part4 7\n",
      "E-step 8\n",
      "E-step part2 \n",
      "M-step 8\n",
      "M-step part1 8\n",
      "M-step part2 8\n",
      "M-step part3 8\n",
      "M-step part4 8\n",
      "E-step 9\n",
      "E-step part2 \n",
      "M-step 9\n",
      "M-step part1 9\n",
      "M-step part2 9\n",
      "M-step part3 9\n",
      "M-step part4 9\n",
      "E-step 10\n",
      "E-step part2 \n",
      "M-step 10\n",
      "M-step part1 10\n",
      "M-step part2 10\n",
      "M-step part3 10\n",
      "M-step part4 10\n",
      "E-step 11\n",
      "E-step part2 \n",
      "M-step 11\n",
      "M-step part1 11\n",
      "M-step part2 11\n",
      "M-step part3 11\n",
      "M-step part4 11\n",
      "E-step 12\n",
      "E-step part2 \n",
      "M-step 12\n",
      "M-step part1 12\n",
      "M-step part2 12\n",
      "M-step part3 12\n",
      "M-step part4 12\n",
      "E-step 13\n",
      "E-step part2 \n",
      "M-step 13\n",
      "M-step part1 13\n",
      "M-step part2 13\n",
      "M-step part3 13\n",
      "M-step part4 13\n",
      "E-step 14\n",
      "E-step part2 \n",
      "M-step 14\n",
      "M-step part1 14\n",
      "M-step part2 14\n",
      "M-step part3 14\n",
      "M-step part4 14\n",
      "E-step 15\n",
      "E-step part2 \n",
      "M-step 15\n",
      "M-step part1 15\n",
      "M-step part2 15\n",
      "M-step part3 15\n",
      "M-step part4 15\n",
      "E-step 16\n",
      "E-step part2 \n",
      "M-step 16\n",
      "M-step part1 16\n",
      "M-step part2 16\n",
      "M-step part3 16\n",
      "M-step part4 16\n",
      "E-step 17\n",
      "E-step part2 \n",
      "M-step 17\n",
      "M-step part1 17\n",
      "M-step part2 17\n",
      "M-step part3 17\n",
      "M-step part4 17\n",
      "E-step 18\n",
      "E-step part2 \n",
      "M-step 18\n",
      "M-step part1 18\n",
      "M-step part2 18\n",
      "M-step part3 18\n",
      "M-step part4 18\n",
      "E-step 19\n",
      "E-step part2 \n",
      "M-step 19\n",
      "M-step part1 19\n",
      "M-step part2 19\n",
      "M-step part3 19\n",
      "M-step part4 19\n",
      "E-step 20\n",
      "E-step part2 \n",
      "M-step 20\n",
      "M-step part1 20\n",
      "M-step part2 20\n",
      "M-step part3 20\n",
      "M-step part4 20\n",
      "E-step 21\n",
      "E-step part2 \n",
      "M-step 21\n",
      "M-step part1 21\n",
      "M-step part2 21\n",
      "M-step part3 21\n",
      "M-step part4 21\n",
      "E-step 22\n",
      "E-step part2 \n",
      "M-step 22\n",
      "M-step part1 22\n",
      "M-step part2 22\n",
      "M-step part3 22\n",
      "M-step part4 22\n",
      "E-step 23\n",
      "E-step part2 \n",
      "M-step 23\n",
      "M-step part1 23\n",
      "M-step part2 23\n",
      "M-step part3 23\n",
      "M-step part4 23\n",
      "E-step 24\n",
      "E-step part2 \n",
      "M-step 24\n",
      "M-step part1 24\n",
      "M-step part2 24\n",
      "M-step part3 24\n",
      "M-step part4 24\n",
      "E-step 25\n",
      "E-step part2 \n",
      "M-step 25\n",
      "M-step part1 25\n",
      "M-step part2 25\n",
      "M-step part3 25\n",
      "M-step part4 25\n",
      "E-step 26\n",
      "E-step part2 \n",
      "M-step 26\n",
      "M-step part1 26\n",
      "M-step part2 26\n",
      "M-step part3 26\n",
      "M-step part4 26\n",
      "E-step 27\n",
      "E-step part2 \n",
      "M-step 27\n",
      "M-step part1 27\n",
      "M-step part2 27\n",
      "M-step part3 27\n",
      "M-step part4 27\n",
      "E-step 28\n",
      "E-step part2 \n",
      "M-step 28\n",
      "M-step part1 28\n",
      "M-step part2 28\n",
      "M-step part3 28\n",
      "M-step part4 28\n",
      "E-step 29\n",
      "E-step part2 \n",
      "M-step 29\n",
      "M-step part1 29\n",
      "M-step part2 29\n",
      "M-step part3 29\n",
      "M-step part4 29\n",
      "E-step 30\n",
      "E-step part2 \n",
      "M-step 30\n",
      "M-step part1 30\n",
      "M-step part2 30\n",
      "M-step part3 30\n",
      "M-step part4 30\n",
      "E-step 31\n",
      "E-step part2 \n",
      "M-step 31\n",
      "M-step part1 31\n",
      "M-step part2 31\n",
      "M-step part3 31\n",
      "M-step part4 31\n",
      "E-step 32\n",
      "E-step part2 \n",
      "M-step 32\n",
      "M-step part1 32\n",
      "M-step part2 32\n",
      "M-step part3 32\n",
      "M-step part4 32\n",
      "E-step 33\n",
      "E-step part2 \n",
      "M-step 33\n",
      "M-step part1 33\n",
      "M-step part2 33\n",
      "M-step part3 33\n",
      "M-step part4 33\n",
      "E-step 34\n",
      "E-step part2 \n",
      "M-step 34\n",
      "M-step part1 34\n",
      "M-step part2 34\n",
      "M-step part3 34\n",
      "M-step part4 34\n",
      "E-step 35\n",
      "E-step part2 \n",
      "M-step 35\n",
      "M-step part1 35\n",
      "M-step part2 35\n",
      "M-step part3 35\n",
      "M-step part4 35\n",
      "E-step 36\n",
      "E-step part2 \n",
      "M-step 36\n",
      "M-step part1 36\n",
      "M-step part2 36\n",
      "M-step part3 36\n",
      "M-step part4 36\n",
      "E-step 37\n",
      "E-step part2 \n",
      "M-step 37\n",
      "M-step part1 37\n",
      "M-step part2 37\n",
      "M-step part3 37\n",
      "M-step part4 37\n",
      "E-step 38\n",
      "E-step part2 \n",
      "M-step 38\n",
      "M-step part1 38\n",
      "M-step part2 38\n",
      "M-step part3 38\n",
      "M-step part4 38\n",
      "E-step 39\n",
      "E-step part2 \n",
      "M-step 39\n",
      "M-step part1 39\n",
      "M-step part2 39\n",
      "M-step part3 39\n",
      "M-step part4 39\n",
      "E-step 40\n",
      "E-step part2 \n",
      "M-step 40\n",
      "M-step part1 40\n",
      "M-step part2 40\n",
      "M-step part3 40\n",
      "M-step part4 40\n",
      "E-step 41\n",
      "E-step part2 \n",
      "M-step 41\n",
      "M-step part1 41\n",
      "M-step part2 41\n",
      "M-step part3 41\n",
      "M-step part4 41\n",
      "E-step 42\n",
      "E-step part2 \n",
      "M-step 42\n",
      "M-step part1 42\n",
      "M-step part2 42\n",
      "M-step part3 42\n",
      "M-step part4 42\n",
      "E-step 43\n",
      "E-step part2 \n",
      "M-step 43\n",
      "M-step part1 43\n",
      "M-step part2 43\n",
      "M-step part3 43\n",
      "M-step part4 43\n",
      "E-step 44\n",
      "E-step part2 \n",
      "M-step 44\n",
      "M-step part1 44\n",
      "M-step part2 44\n",
      "M-step part3 44\n",
      "M-step part4 44\n",
      "E-step 45\n",
      "E-step part2 \n",
      "M-step 45\n",
      "M-step part1 45\n",
      "M-step part2 45\n",
      "M-step part3 45\n",
      "M-step part4 45\n",
      "E-step 46\n",
      "E-step part2 \n",
      "M-step 46\n",
      "M-step part1 46\n",
      "M-step part2 46\n",
      "M-step part3 46\n",
      "M-step part4 46\n",
      "E-step 47\n",
      "E-step part2 \n",
      "M-step 47\n",
      "M-step part1 47\n",
      "M-step part2 47\n",
      "M-step part3 47\n",
      "M-step part4 47\n",
      "E-step 48\n",
      "E-step part2 \n",
      "M-step 48\n",
      "M-step part1 48\n",
      "M-step part2 48\n",
      "M-step part3 48\n",
      "M-step part4 48\n",
      "E-step 49\n",
      "E-step part2 \n",
      "M-step 49\n",
      "M-step part1 49\n",
      "M-step part2 49\n",
      "M-step part3 49\n",
      "M-step part4 49\n",
      "E-step 50\n",
      "E-step part2 \n",
      "M-step 50\n",
      "M-step part1 50\n",
      "M-step part2 50\n",
      "M-step part3 50\n",
      "M-step part4 50\n",
      "E-step 51\n",
      "E-step part2 \n",
      "M-step 51\n",
      "M-step part1 51\n",
      "M-step part2 51\n",
      "M-step part3 51\n",
      "M-step part4 51\n",
      "E-step 52\n",
      "E-step part2 \n",
      "M-step 52\n",
      "M-step part1 52\n",
      "M-step part2 52\n",
      "M-step part3 52\n",
      "M-step part4 52\n",
      "E-step 53\n",
      "E-step part2 \n",
      "M-step 53\n",
      "M-step part1 53\n",
      "M-step part2 53\n",
      "M-step part3 53\n",
      "M-step part4 53\n",
      "E-step 54\n",
      "E-step part2 \n",
      "M-step 54\n",
      "M-step part1 54\n",
      "M-step part2 54\n",
      "M-step part3 54\n",
      "M-step part4 54\n",
      "E-step 55\n",
      "E-step part2 \n",
      "M-step 55\n",
      "M-step part1 55\n",
      "M-step part2 55\n",
      "M-step part3 55\n",
      "M-step part4 55\n",
      "E-step 56\n",
      "E-step part2 \n",
      "M-step 56\n",
      "M-step part1 56\n",
      "M-step part2 56\n",
      "M-step part3 56\n",
      "M-step part4 56\n",
      "E-step 57\n",
      "E-step part2 \n",
      "M-step 57\n",
      "M-step part1 57\n",
      "M-step part2 57\n",
      "M-step part3 57\n",
      "M-step part4 57\n",
      "E-step 58\n",
      "E-step part2 \n",
      "M-step 58\n",
      "M-step part1 58\n",
      "M-step part2 58\n",
      "M-step part3 58\n",
      "M-step part4 58\n",
      "E-step 59\n",
      "E-step part2 \n",
      "M-step 59\n",
      "M-step part1 59\n",
      "M-step part2 59\n",
      "M-step part3 59\n",
      "M-step part4 59\n",
      "E-step 60\n",
      "E-step part2 \n",
      "M-step 60\n",
      "M-step part1 60\n",
      "M-step part2 60\n",
      "M-step part3 60\n",
      "M-step part4 60\n",
      "E-step 61\n",
      "E-step part2 \n",
      "M-step 61\n",
      "M-step part1 61\n",
      "M-step part2 61\n",
      "M-step part3 61\n",
      "M-step part4 61\n",
      "E-step 62\n",
      "E-step part2 \n",
      "M-step 62\n",
      "M-step part1 62\n",
      "M-step part2 62\n",
      "M-step part3 62\n",
      "M-step part4 62\n",
      "E-step 63\n",
      "E-step part2 \n",
      "M-step 63\n",
      "M-step part1 63\n",
      "M-step part2 63\n",
      "M-step part3 63\n",
      "M-step part4 63\n",
      "E-step 64\n",
      "E-step part2 \n",
      "M-step 64\n",
      "M-step part1 64\n",
      "M-step part2 64\n",
      "M-step part3 64\n",
      "M-step part4 64\n",
      "E-step 65\n",
      "E-step part2 \n",
      "M-step 65\n",
      "M-step part1 65\n",
      "M-step part2 65\n",
      "M-step part3 65\n",
      "M-step part4 65\n",
      "E-step 66\n",
      "E-step part2 \n",
      "M-step 66\n",
      "M-step part1 66\n",
      "M-step part2 66\n",
      "M-step part3 66\n",
      "M-step part4 66\n",
      "E-step 67\n",
      "E-step part2 \n",
      "M-step 67\n",
      "M-step part1 67\n",
      "M-step part2 67\n",
      "M-step part3 67\n",
      "M-step part4 67\n",
      "E-step 68\n",
      "E-step part2 \n",
      "M-step 68\n",
      "M-step part1 68\n",
      "M-step part2 68\n",
      "M-step part3 68\n",
      "M-step part4 68\n",
      "E-step 69\n",
      "E-step part2 \n",
      "M-step 69\n",
      "M-step part1 69\n",
      "M-step part2 69\n",
      "M-step part3 69\n",
      "M-step part4 69\n",
      "E-step 70\n",
      "E-step part2 \n",
      "M-step 70\n",
      "M-step part1 70\n",
      "M-step part2 70\n",
      "M-step part3 70\n",
      "M-step part4 70\n",
      "E-step 71\n",
      "E-step part2 \n",
      "M-step 71\n",
      "M-step part1 71\n",
      "M-step part2 71\n",
      "M-step part3 71\n",
      "M-step part4 71\n",
      "E-step 72\n",
      "E-step part2 \n",
      "M-step 72\n",
      "M-step part1 72\n",
      "M-step part2 72\n",
      "M-step part3 72\n",
      "M-step part4 72\n",
      "E-step 73\n",
      "E-step part2 \n",
      "M-step 73\n",
      "M-step part1 73\n",
      "M-step part2 73\n",
      "M-step part3 73\n",
      "M-step part4 73\n",
      "E-step 74\n",
      "E-step part2 \n",
      "M-step 74\n",
      "M-step part1 74\n",
      "M-step part2 74\n",
      "M-step part3 74\n",
      "M-step part4 74\n",
      "E-step 75\n",
      "E-step part2 \n",
      "M-step 75\n",
      "M-step part1 75\n",
      "M-step part2 75\n",
      "M-step part3 75\n",
      "M-step part4 75\n",
      "E-step 76\n",
      "E-step part2 \n",
      "M-step 76\n",
      "M-step part1 76\n",
      "M-step part2 76\n",
      "M-step part3 76\n",
      "M-step part4 76\n",
      "E-step 77\n",
      "E-step part2 \n",
      "M-step 77\n",
      "M-step part1 77\n",
      "M-step part2 77\n",
      "M-step part3 77\n",
      "M-step part4 77\n",
      "E-step 78\n",
      "E-step part2 \n",
      "M-step 78\n",
      "M-step part1 78\n",
      "M-step part2 78\n",
      "M-step part3 78\n",
      "M-step part4 78\n",
      "E-step 79\n",
      "E-step part2 \n",
      "M-step 79\n",
      "M-step part1 79\n",
      "M-step part2 79\n",
      "M-step part3 79\n",
      "M-step part4 79\n",
      "E-step 80\n",
      "E-step part2 \n",
      "M-step 80\n",
      "M-step part1 80\n",
      "M-step part2 80\n",
      "M-step part3 80\n",
      "M-step part4 80\n",
      "E-step 81\n",
      "E-step part2 \n",
      "M-step 81\n",
      "M-step part1 81\n",
      "M-step part2 81\n",
      "M-step part3 81\n",
      "M-step part4 81\n",
      "E-step 82\n",
      "E-step part2 \n",
      "M-step 82\n",
      "M-step part1 82\n",
      "M-step part2 82\n",
      "M-step part3 82\n",
      "M-step part4 82\n",
      "E-step 83\n",
      "E-step part2 \n",
      "M-step 83\n",
      "M-step part1 83\n",
      "M-step part2 83\n",
      "M-step part3 83\n",
      "M-step part4 83\n",
      "E-step 84\n",
      "E-step part2 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M-step 84\n",
      "M-step part1 84\n",
      "M-step part2 84\n",
      "M-step part3 84\n",
      "M-step part4 84\n",
      "E-step 85\n",
      "E-step part2 \n",
      "M-step 85\n",
      "M-step part1 85\n",
      "M-step part2 85\n",
      "M-step part3 85\n",
      "M-step part4 85\n",
      "E-step 86\n",
      "E-step part2 \n",
      "M-step 86\n",
      "M-step part1 86\n",
      "M-step part2 86\n",
      "M-step part3 86\n",
      "M-step part4 86\n",
      "E-step 87\n",
      "E-step part2 \n",
      "M-step 87\n",
      "M-step part1 87\n",
      "M-step part2 87\n",
      "M-step part3 87\n",
      "M-step part4 87\n",
      "E-step 88\n",
      "E-step part2 \n",
      "M-step 88\n",
      "M-step part1 88\n",
      "M-step part2 88\n",
      "M-step part3 88\n",
      "M-step part4 88\n",
      "E-step 89\n",
      "E-step part2 \n",
      "M-step 89\n",
      "M-step part1 89\n",
      "M-step part2 89\n",
      "M-step part3 89\n",
      "M-step part4 89\n",
      "E-step 90\n",
      "E-step part2 \n",
      "M-step 90\n",
      "M-step part1 90\n",
      "M-step part2 90\n",
      "M-step part3 90\n",
      "M-step part4 90\n",
      "E-step 91\n",
      "E-step part2 \n",
      "M-step 91\n",
      "M-step part1 91\n",
      "M-step part2 91\n",
      "M-step part3 91\n",
      "M-step part4 91\n",
      "E-step 92\n",
      "E-step part2 \n",
      "M-step 92\n",
      "M-step part1 92\n",
      "M-step part2 92\n",
      "M-step part3 92\n",
      "M-step part4 92\n",
      "E-step 93\n",
      "E-step part2 \n",
      "M-step 93\n",
      "M-step part1 93\n",
      "M-step part2 93\n",
      "M-step part3 93\n",
      "M-step part4 93\n",
      "E-step 94\n",
      "E-step part2 \n",
      "M-step 94\n",
      "M-step part1 94\n",
      "M-step part2 94\n",
      "M-step part3 94\n",
      "M-step part4 94\n",
      "E-step 95\n",
      "E-step part2 \n",
      "M-step 95\n",
      "M-step part1 95\n",
      "M-step part2 95\n",
      "M-step part3 95\n",
      "M-step part4 95\n",
      "E-step 96\n",
      "E-step part2 \n",
      "M-step 96\n",
      "M-step part1 96\n",
      "M-step part2 96\n",
      "M-step part3 96\n",
      "M-step part4 96\n",
      "E-step 97\n",
      "E-step part2 \n",
      "M-step 97\n",
      "M-step part1 97\n",
      "M-step part2 97\n",
      "M-step part3 97\n",
      "M-step part4 97\n",
      "E-step 98\n",
      "E-step part2 \n",
      "M-step 98\n",
      "M-step part1 98\n",
      "M-step part2 98\n",
      "M-step part3 98\n",
      "M-step part4 98\n",
      "E-step 99\n",
      "E-step part2 \n",
      "M-step 99\n",
      "M-step part1 99\n",
      "M-step part2 99\n",
      "M-step part3 99\n",
      "M-step part4 99\n"
     ]
    }
   ],
   "source": [
    "# Try out the functions we just defined on the data\n",
    "n_components=10\n",
    "n_emiterations=100\n",
    "mixturemodel_weights,mixturemodel_means,mixturemodel_covariances,mixturemodel_inversecovariances = perform_emalgorithm(\n",
    "    X,n_components,n_emiterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "coated-violin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "'' winged `` command three cap time say bow\n",
      "1\n",
      "`` answer try lion one say come woodman give\n",
      "2\n",
      "tin '' `` scarecrow say 've axe lion --\n",
      "3\n",
      "`` ask scarecrow dorothy say go woodman tin brain\n",
      "4\n",
      "tree dorothy look come scarecrow long house wait sit\n",
      "5\n",
      "'' ask dorothy say scarecrow man oz get answer\n",
      "6\n",
      "soldier gate dorothy girl whisker dress little city emerald\n",
      "7\n",
      "`` lion courage say cowardly give oz woodman promise\n",
      "8\n",
      "wicked winkies `` '' west east tell silver country\n",
      "9\n",
      "`` say dorothy oz n't answer great go must\n"
     ]
    }
   ],
   "source": [
    "# Find top 10 words for each cluster\n",
    "for k in range(n_components):\n",
    "    print(k)\n",
    "    highest_dimensionweight_indices=np.argsort(-np.squeeze(mixturemodel_means[k,:].toarray()),axis=0)\n",
    "\n",
    "    print(' '.join(remaining_vocabulary[highest_dimensionweight_indices[1:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-studio",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "mathematical-mainland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 815  803  886  883  798  656  260  797  359 1145]\n",
      "So they sat down and listened while he told the following tale :\n",
      "1\n",
      "[ 797  945  954  883  101  733   67 1130  412  798]\n",
      "`` Not a bit of it , my dear ; I 'm just a common man . ''\n",
      "2\n",
      "[803 683 100 883 469 101 733  67 738 798]\n",
      "`` Exactly so ! I am a humbug . ''\n",
      "3\n",
      "[ 886  556  816  883  101  733   67  195  916 1037]\n",
      "The Lion hesitated no longer , but drank till the dish was empty .\n",
      "4\n",
      "[803 733 412 798 955 813 756 216 569 894]\n",
      "`` Exactly so ! I am a humbug . ''\n",
      "5\n",
      "[ 589  980  803  864  556  990  816 1068 1024  306]\n",
      "[ Illustration : `` _The Soldier with the green whiskers led them through the streets._ '' ]\n",
      "6\n",
      "[ 883  733  798 1006  306  571  756  995  216 1021]\n",
      "`` Drink . ''\n",
      "7\n",
      "[ 803  954 1024  101  733   67  195  738  798 1101]\n",
      "`` Exactly so ! I am a humbug . ''\n",
      "8\n",
      "[1028  886  883  733  412  798  656  955 1088  724]\n",
      "`` Do n't chase me ! do n't chase me ! ''\n",
      "9\n",
      "[  97  886  883  101  195  412  906  565 1068  492]\n",
      "When Boq saw her silver shoes he said ,\n"
     ]
    }
   ],
   "source": [
    "# Version 2 - Get documents closest to component mean, i.e. highest p(d|k).\n",
    "# ---The computation of distances here is the same as done in the E-step of EM---\n",
    "# For each component, compute terms that do not involve data\n",
    "meanterms=np.zeros((n_components))\n",
    "logdeterminants=np.zeros((n_components))\n",
    "logconstantterms=np.zeros((n_components))\n",
    "for k in range(n_components):\n",
    "    # Compute mu_k*inv(Sigma_k)*mu_k\n",
    "    meanterms[k]=(mixturemodel_means[k,:]*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T)[0,0]\n",
    "\n",
    "# Compute terms that involve distances of data from components\n",
    "xnorms=np.zeros((n_data,n_components))\n",
    "xtimesmu=np.zeros((n_data,n_components))\n",
    "\n",
    "for k in range(n_components):\n",
    "    xnorms[:,k]=(X*mixturemodel_inversecovariances[k]*X.T).diagonal(0)\n",
    "    xtimesmu[:,k]=np.squeeze((X*mixturemodel_inversecovariances[k]*mixturemodel_means[k,:].T).toarray())\n",
    "\n",
    "xdists=xnorms+np.matlib.repmat(meanterms,n_data,1)-2*xtimesmu\n",
    "\n",
    "for k in range(n_components):\n",
    "    tempdists=np.array(np.squeeze(xdists[:,k]))\n",
    "    highest_componentprob_indices=np.argsort(-1*tempdists,axis=0)\n",
    "    print(k)\n",
    "    print(highest_componentprob_indices[0:10])\n",
    "    print(' '.join(mydownloaded_nltk_texts[highest_componentprob_indices[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-saskatchewan",
   "metadata": {},
   "source": [
    "#### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-summit",
   "metadata": {},
   "source": [
    "* The paragraph in each mixture component having the highest membership probability doesn't match the corresponding top-10 words in the mixture component. This could be as a result of the fact that the paragraph might be near the center of the cluster since it is composed of generic words that may be common enough in every cluster. \n",
    "* More so, some paragraphs with the highest membership probability gave the same result.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-category",
   "metadata": {},
   "source": [
    "#### Exercise 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "critical-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the number of words in each paragraph\n",
    "word_count = np.zeros((len(mydownloaded_lemmatizedtexts), 1))\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    count = len(mydownloaded_lemmatizedtexts[k])\n",
    "    word_count[k] = count\n",
    "    #print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "champion-sailing",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_paragraphindex = np.argsort(-1*word_count, axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "modified-berkeley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([584], dtype=int64), 233)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longest paragraph index and number of words in the paragraph\n",
    "longest_paragraphindex, len(mydownloaded_lemmatizedtexts[584])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "experimental-wings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "she leave dorothy alone and go back to the others . these she also lead to room , and each one of them find himself lodge in a very pleasant part of the palace . of course this politeness be waste on the scarecrow ; for when he find himself alone in his room he stand stupidly in one spot , just within the doorway , to wait till morning . it would not rest him to lie down , and he could not close his eye ; so he remain all night star at a little spider which be weave its web in a corner of the room , just as if it be not one of the most wonderful room in the world . the tin woodman lay down on his bed from force of habit , for he remember when he be make of flesh ; but not be able to sleep he pass the night move his joint up and down to make sure they keep in good work order . the lion would have prefer a bed of dried leaf in the forest , and do not like be shut up in a room ; but he have too much sense to let this worry him , so he spring upon the bed and roll himself up like a cat and purr himself asleep in a minute .\n"
     ]
    }
   ],
   "source": [
    "# longest paragraph\n",
    "print(' '.join(mydownloaded_lemmatizedtexts[584]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-statement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
