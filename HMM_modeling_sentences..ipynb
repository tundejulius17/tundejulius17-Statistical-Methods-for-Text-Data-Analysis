{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "joint-letter",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercise 8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "placed-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import hmmlearn, hmmlearn.hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "behavioral-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file = open('hmm_sentences.txt','r', encoding='utf-8', errors='ignore')\n",
    "text_lines = temp_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "reverse-entrance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "printable-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_sentences = []\n",
    "for line in text_lines:\n",
    "    temp_text = line.strip()\n",
    "    temp_text = ' '.join(temp_text.split())\n",
    "    hmm_sentences.append(temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "attractive-chart",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'where may you feed ?'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "advised-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize downloaded texts and change them to NLTK format\n",
    "mydownloaded_nltk_texts = []\n",
    "for k in range(len(hmm_sentences)):\n",
    "    temp_tokenizedtext = nltk.word_tokenize(hmm_sentences[k])\n",
    "    temp_nltktext = nltk.Text(temp_tokenizedtext)\n",
    "    mydownloaded_nltk_texts.append(temp_nltktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "vertical-alias",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydownloaded_nltk_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "retained-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all downloaded texts lowercase\n",
    "mydownloaded_lowercase_texts = []\n",
    "for k in range(len(mydownloaded_nltk_texts)):\n",
    "    temp_lowercase_text = []\n",
    "    for l in range(len(mydownloaded_nltk_texts[k])):\n",
    "        lowercase_word = mydownloaded_nltk_texts[k][l].lower()\n",
    "        temp_lowercase_text.append(lowercase_word)\n",
    "    temp_lowercasetest = nltk.Text(temp_lowercase_text)\n",
    "    mydownloaded_lowercase_texts.append(temp_lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "consolidated-uganda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mydownloaded_lowercase_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "closing-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a POS tag for WordNet\n",
    "def tagtowordnet(postag):\n",
    "    wordnettag=-1\n",
    "    if postag[0]=='N':\n",
    "        wordnettag='n'\n",
    "    elif postag[0]=='V':\n",
    "        wordnettag='v'\n",
    "    elif postag[0]=='J':\n",
    "        wordnettag='a'\n",
    "    elif postag[0]=='R':\n",
    "        wordnettag='r'\n",
    "    return(wordnettag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "narrative-birth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# POS tag and lemmatize the loaded texts\n",
    "# Download tagger and wordnet resources if you do not have them already\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatize downloaded texts\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):\n",
    "    # tag the text with POS tags\n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)\n",
    "    # lemmatize each word text\n",
    "    lemmatizedtext = []\n",
    "    for l in range(len(taggedtext)):\n",
    "        # Lemmatize a word using the WordNet converted POS tag\n",
    "        wordtolemmatize = taggedtext[l][0]\n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])\n",
    "        if wordnettag != -1:\n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize, wordnettag)\n",
    "        else:\n",
    "            lemmatizedword = wordtolemmatize\n",
    "        # store the lemmatized word\n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "    return(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "whole-major",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydownloaded_lemmatizedtexts = []\n",
    "for k in range(len(mydownloaded_lowercase_texts)):\n",
    "    lemmatizedtext = lemmatizetext(mydownloaded_lowercase_texts[k])\n",
    "    lemmatizedtext = nltk.Text(lemmatizedtext)\n",
    "    mydownloaded_lemmatizedtexts.append(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "covered-community",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: where may you fee ?...>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydownloaded_lemmatizedtexts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "printable-morning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the vocabularies of each document\n",
    "myvocabularies = []\n",
    "myindices_in_vocabularies = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # get unique words and where they occur\n",
    "    temptext = mydownloaded_lemmatizedtexts[k]\n",
    "    unique_results = np.unique(temptext, return_inverse = True)\n",
    "    unique_words = unique_results[0]\n",
    "    word_indices = unique_results[1]\n",
    "    # store the vocabularies and the indices\n",
    "    myvocabularies.append(unique_words)\n",
    "    myindices_in_vocabularies.append(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "special-laugh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myvocabularies), len(myindices_in_vocabularies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "inner-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the vocabularies\n",
    "# first concatenate all vocabularies\n",
    "tempvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "\n",
    "# find unique words among all the vocabularies\n",
    "uniqueresults = np.unique(tempvocabulary, return_inverse = True)\n",
    "unifiedvocabulary = uniqueresults[0]\n",
    "wordindices = uniqueresults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "polyphonic-madness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "frozen-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate previous indices to the unified vocabulary\n",
    "# must keep track where each vocabulary started in the concatenated one\n",
    "vocabularystart = 0\n",
    "myindices_in_unifiedvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # in order to shift word indices, we must temporarily\n",
    "    # convert their data type to a numpy array\n",
    "    tempindices = np.array(myindices_in_vocabularies[k])\n",
    "    tempindices = tempindices + vocabularystart\n",
    "    tempindices = wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart = vocabularystart + len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "extended-tract",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(myindices_in_unifiedvocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "satisfactory-interference",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([38, 27, 42, 15,  2], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordindices[np.array(myindices_in_vocabularies[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "unlike-knife",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 15, 27, ..., 37, 40, 41], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordindices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "preceding-surgery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', '.', '?', 'a', 'and', 'answer', 'ask', 'be', 'bird', 'but',\n",
       "       'can', 'cat', 'cautious', 'dog', 'explain', 'fee', 'feed', 'fox',\n",
       "       'friendly', 'he', 'help', 'her', 'him', 'how', 'however', 'i',\n",
       "       'insightful', 'may', 'me', 'robot', 'she', 'small', 'strong',\n",
       "       'the', 'them', 'then', 'they', 'when', 'where', 'why', 'will',\n",
       "       'wise', 'you'], dtype='<U10')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unifiedvocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "visible-taxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['where', 'may', 'you', 'fee', '?'], dtype='<U10')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unifiedvocabulary[myindices_in_unifiedvocabulary[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "traditional-lithuania",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data = []\n",
    "documentlengths = []\n",
    "for k in range(len(myindices_in_unifiedvocabulary)):\n",
    "    concatenated_data.extend(myindices_in_unifiedvocabulary[k])\n",
    "    documentlengths.append(len(myindices_in_unifiedvocabulary[k]))\n",
    "    \n",
    "concatenated_data = np.matrix(concatenated_data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "wooden-weekend",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "         1      -66665.2226             +nan\n",
      "         2      -60311.2848       +6353.9378\n",
      "         3      -60288.9410         +22.3438\n",
      "         4      -60257.8481         +31.0929\n",
      "         5      -60209.6261         +48.2220\n",
      "         6      -60131.3479         +78.2782\n",
      "         7      -60002.8016        +128.5464\n",
      "         8      -59792.3515        +210.4501\n",
      "         9      -59448.0693        +344.2822\n",
      "        10      -58884.7309        +563.3384\n",
      "        11      -57999.7767        +884.9542\n",
      "        12      -56768.1312       +1231.6456\n",
      "        13      -55343.4960       +1424.6352\n",
      "        14      -54047.9964       +1295.4995\n",
      "        15      -53083.8300        +964.1665\n",
      "        16      -52401.7691        +682.0609\n",
      "        17      -51926.4940        +475.2751\n",
      "        18      -51620.8999        +305.5941\n",
      "        19      -51435.8438        +185.0561\n",
      "        20      -51283.8640        +151.9799\n",
      "        21      -51079.5148        +204.3492\n",
      "        22      -50774.0453        +305.4695\n",
      "        23      -50439.9613        +334.0840\n",
      "        24      -50178.3547        +261.6066\n",
      "        25      -50020.0808        +158.2739\n",
      "        26      -49936.4999         +83.5809\n",
      "        27      -49892.3205         +44.1794\n",
      "        28      -49867.2292         +25.0912\n",
      "        29      -49849.5956         +17.6336\n",
      "        30      -49832.7855         +16.8101\n",
      "        31      -49812.2522         +20.5333\n",
      "        32      -49782.9765         +29.2758\n",
      "        33      -49738.6055         +44.3710\n",
      "        34      -49674.3689         +64.2366\n",
      "        35      -49593.7741         +80.5948\n",
      "        36      -49509.9084         +83.8657\n",
      "        37      -49438.0595         +71.8489\n",
      "        38      -49389.4078         +48.6517\n",
      "        39      -49362.4244         +26.9834\n",
      "        40      -49346.6090         +15.8154\n",
      "        41      -49333.3995         +13.2095\n",
      "        42      -49318.5102         +14.8894\n",
      "        43      -49300.8268         +17.6833\n",
      "        44      -49281.7040         +19.1228\n",
      "        45      -49263.9391         +17.7649\n",
      "        46      -49249.8731         +14.0660\n",
      "        47      -49240.0756          +9.7975\n",
      "        48      -49233.7178          +6.3578\n",
      "        49      -49229.6176          +4.1002\n",
      "        50      -49226.8121          +2.8055\n",
      "        51      -49224.6308          +2.1813\n",
      "        52      -49222.6225          +2.0083\n",
      "        53      -49220.5232          +2.0993\n",
      "        54      -49218.2654          +2.2578\n",
      "        55      -49215.9422          +2.3231\n",
      "        56      -49213.7111          +2.2312\n",
      "        57      -49211.7088          +2.0023\n",
      "        58      -49210.0099          +1.6989\n",
      "        59      -49208.6169          +1.3930\n",
      "        60      -49207.4759          +1.1409\n",
      "        61      -49206.5022          +0.9737\n",
      "        62      -49205.5970          +0.9051\n",
      "        63      -49204.6504          +0.9466\n",
      "        64      -49203.5308          +1.1196\n",
      "        65      -49202.0653          +1.4655\n",
      "        66      -49200.0097          +2.0556\n",
      "        67      -49197.0066          +3.0032\n",
      "        68      -49192.5344          +4.4722\n",
      "        69      -49185.8783          +6.6561\n",
      "        70      -49176.2283          +9.6500\n",
      "        71      -49163.1302         +13.0981\n",
      "        72      -49147.3682         +15.7620\n",
      "        73      -49131.3826         +15.9856\n",
      "        74      -49117.8327         +13.5499\n",
      "        75      -49107.6945         +10.1382\n",
      "        76      -49100.4782          +7.2162\n",
      "        77      -49095.4132          +5.0651\n",
      "        78      -49091.9119          +3.5012\n",
      "        79      -49089.5487          +2.3633\n",
      "        80      -49087.9956          +1.5531\n",
      "        81      -49086.9993          +0.9962\n",
      "        82      -49086.3725          +0.6268\n",
      "        83      -49085.9835          +0.3890\n",
      "        84      -49085.7443          +0.2393\n",
      "        85      -49085.5977          +0.1465\n",
      "        86      -49085.5082          +0.0896\n",
      "        87      -49085.4534          +0.0548\n",
      "        88      -49085.4198          +0.0336\n",
      "        89      -49085.3991          +0.0207\n",
      "        90      -49085.3863          +0.0128\n",
      "        91      -49085.3783          +0.0080\n"
     ]
    }
   ],
   "source": [
    "myhmm = hmmlearn.hmm.MultinomialHMM(n_components=5, n_iter=100, verbose=True)\n",
    "myhmm_fitted = myhmm.fit(concatenated_data, lengths = documentlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "stuck-sally",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.14000000e-01, 8.56572050e-42, 2.01843053e-53, 7.86000000e-01,\n",
       "       0.00000000e+00])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.startprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "filled-costa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.startprob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "coral-cherry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.64693672e-045, 5.25453865e-003, 9.91775680e-002,\n",
       "        1.24877844e-001, 1.54366359e-002, 7.49706281e-003,\n",
       "        1.20864044e-003, 4.16285490e-025, 5.63279856e-002,\n",
       "        1.39802856e-086, 6.30558772e-097, 1.89802845e-002,\n",
       "        3.21642003e-002, 2.33436817e-002, 4.41693451e-003,\n",
       "        3.59838959e-003, 5.87738062e-003, 6.30901419e-002,\n",
       "        4.31877230e-002, 1.67948955e-063, 1.55015967e-003,\n",
       "        9.74321718e-045, 3.75274990e-028, 0.00000000e+000,\n",
       "        3.09023929e-125, 8.16861601e-059, 8.06868156e-002,\n",
       "        2.50699022e-105, 9.23081051e-019, 4.83789281e-002,\n",
       "        1.82707618e-057, 3.25783510e-002, 9.90823463e-002,\n",
       "        1.93499319e-001, 7.87244682e-048, 2.54541887e-061,\n",
       "        2.04171891e-060, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 3.45707663e-107, 3.97850688e-002,\n",
       "        2.89071165e-017],\n",
       "       [3.13880641e-001, 2.10120914e-002, 1.01610351e-001,\n",
       "        1.65146348e-002, 1.61519277e-001, 1.03998727e-061,\n",
       "        1.78079002e-070, 5.20371519e-083, 5.42407276e-025,\n",
       "        1.41849905e-001, 1.04945941e-277, 1.44197803e-019,\n",
       "        2.37081912e-035, 2.20865796e-023, 9.95086194e-061,\n",
       "        1.03390662e-083, 5.48976170e-019, 3.77476699e-025,\n",
       "        2.74214006e-044, 0.00000000e+000, 4.83274991e-061,\n",
       "        6.03616618e-003, 5.09301521e-003, 0.00000000e+000,\n",
       "        8.60153680e-002, 0.00000000e+000, 7.29940001e-039,\n",
       "        1.12712963e-288, 1.01860304e-002, 6.68020179e-024,\n",
       "        0.00000000e+000, 4.25619185e-038, 3.52114727e-039,\n",
       "        2.01107857e-002, 5.65890579e-003, 9.92194815e-002,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.90806661e-290, 1.07404479e-035,\n",
       "        1.12933465e-002],\n",
       "       [0.00000000e+000, 1.50979969e-039, 1.50007168e-049,\n",
       "        2.45098883e-006, 2.87019576e-089, 6.63351115e-080,\n",
       "        4.25261269e-110, 3.13569009e-001, 3.72813339e-085,\n",
       "        0.00000000e+000, 9.38656841e-002, 5.33958315e-090,\n",
       "        1.45649285e-180, 6.87709043e-077, 1.35808591e-093,\n",
       "        8.83547538e-089, 1.07403136e-061, 4.46146055e-088,\n",
       "        1.70988852e-177, 1.93097423e-002, 6.93615505e-066,\n",
       "        0.00000000e+000, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.04274749e-001, 9.68147259e-182,\n",
       "        1.83365522e-001, 0.00000000e+000, 2.42294691e-095,\n",
       "        1.09467653e-002, 2.28528917e-181, 1.21458517e-173,\n",
       "        1.82215462e-006, 0.00000000e+000, 0.00000000e+000,\n",
       "        4.69883520e-002, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.36900450e-001, 3.30055799e-171,\n",
       "        9.07754532e-002],\n",
       "       [1.96805459e-053, 2.48628741e-051, 6.30212856e-041,\n",
       "        2.34897180e-271, 5.64962588e-033, 2.99226605e-117,\n",
       "        5.50810116e-138, 2.50404470e-001, 4.10438484e-002,\n",
       "        3.33549915e-034, 0.00000000e+000, 1.67690513e-002,\n",
       "        0.00000000e+000, 1.97705085e-002, 9.05967583e-144,\n",
       "        1.27373480e-112, 2.39451439e-110, 4.94734836e-002,\n",
       "        0.00000000e+000, 2.91214422e-002, 2.78108160e-108,\n",
       "        0.00000000e+000, 0.00000000e+000, 2.49612362e-002,\n",
       "        0.00000000e+000, 1.09445420e-001, 0.00000000e+000,\n",
       "        0.00000000e+000, 0.00000000e+000, 5.11677975e-002,\n",
       "        1.79208875e-002, 0.00000000e+000, 0.00000000e+000,\n",
       "        2.51740257e-269, 0.00000000e+000, 6.39957244e-011,\n",
       "        8.12840255e-002, 7.16835500e-002, 4.96024565e-002,\n",
       "        2.27211252e-002, 0.00000000e+000, 0.00000000e+000,\n",
       "        1.64630697e-001],\n",
       "       [1.55614181e-088, 7.89083554e-096, 1.08082517e-056,\n",
       "        4.55806152e-038, 1.03406197e-114, 9.59310518e-002,\n",
       "        5.86331193e-002, 2.83311413e-177, 7.32183830e-037,\n",
       "        0.00000000e+000, 2.25622155e-245, 9.43171770e-036,\n",
       "        6.83278687e-002, 2.50202967e-034, 9.71115060e-002,\n",
       "        9.41763991e-002, 6.15159408e-003, 4.40135157e-039,\n",
       "        6.90609264e-002, 1.80547338e-003, 4.61407105e-002,\n",
       "        3.66308850e-148, 1.29065373e-111, 0.00000000e+000,\n",
       "        0.00000000e+000, 1.05291277e-002, 1.42409236e-001,\n",
       "        4.70593098e-257, 1.96124914e-092, 2.95102464e-037,\n",
       "        3.18812448e-004, 5.71707893e-002, 1.53201027e-001,\n",
       "        1.07727834e-034, 1.97436192e-130, 9.63523636e-290,\n",
       "        7.93457186e-003, 0.00000000e+000, 0.00000000e+000,\n",
       "        0.00000000e+000, 9.72122741e-260, 7.73792700e-002,\n",
       "        1.37185164e-002]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.emissionprob_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "laden-trustee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(myhmm_fitted.emissionprob_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "expected-letter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 43)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.emissionprob_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "hydraulic-handbook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.29069521e-001 1.12538935e-001 9.03278504e-018 1.37186879e-001\n",
      "  2.12046653e-002]\n",
      " [3.33272551e-002 5.52345563e-001 1.05265739e-041 4.14327182e-001\n",
      "  7.20614422e-101]\n",
      " [5.08035868e-002 4.13554787e-116 9.17364624e-002 6.89492648e-179\n",
      "  8.57459951e-001]\n",
      " [6.79418411e-002 6.05357484e-054 9.32058159e-001 1.72491393e-178\n",
      "  1.38518187e-061]\n",
      " [1.48840115e-001 8.51159885e-001 9.58551598e-110 5.94279304e-090\n",
      "  1.38574006e-045]]\n"
     ]
    }
   ],
   "source": [
    "print(myhmm_fitted.transmat_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fitted-scroll",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(myhmm_fitted.transmat_, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "present-venice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myhmm_fitted.transmat_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-smell",
   "metadata": {},
   "source": [
    "Yes, the states correspond to meaningful properties of the simplified language.\n",
    "* The emission probabilities of each state sum to 1.\n",
    "* The transition probabilities of each state also sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-friendly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
