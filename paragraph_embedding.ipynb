{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nervous-digit",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adequate-commission",
   "metadata": {},
   "source": [
    "##### Exercise 12.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "toxic-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import gensim\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abandoned-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Getting a list of directory contents\n",
    "def gettextlist(directory_path):\n",
    "    directory_textfiles=[]\n",
    "    directory_nontextfiles=[]\n",
    "    directory_nonfiles=[]\n",
    "    # Process each item in the directory\n",
    "    directory_contents=os.listdir(directory_path)\n",
    "    for contentitem in directory_contents:\n",
    "        temp_fullpath=os.path.join(directory_path, contentitem)\n",
    "        # Non-files (e.g. subdirectories) are stored separately\n",
    "        if os.path.isfile(temp_fullpath)==0:\n",
    "            directory_nonfiles.append(contentitem)\n",
    "        else:\n",
    "            # Is this a non-text file (not ending in .txt)?\n",
    "            if temp_fullpath.find('.txt')==-1:\n",
    "                directory_nontextfiles.append(contentitem)\n",
    "            else:\n",
    "                # This is a text file\n",
    "                directory_textfiles.append(contentitem)\n",
    "    return(directory_textfiles,directory_nontextfiles,directory_nonfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "identical-patient",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Basic file crawler\n",
    "def basicfilecrawler(directory_path):\n",
    "    # Store filenames read and their text content\n",
    "    num_files_read=0\n",
    "    crawled_filenames=[]\n",
    "    mycrawled_texts=[]\n",
    "    directory_contentlists=gettextlist(directory_path)\n",
    "    # In this basic crawled we just process text files\n",
    "    # and do not handle subdirectories\n",
    "    #directory_textfiles=directory_contentlists[0]\n",
    "    directory_nontextfiles=directory_contentlists[1]\n",
    "    for contentitem in directory_nontextfiles:\n",
    "        #print('Reading file:')\n",
    "        #print(contentitem)\n",
    "        # Open the file and read its contents\n",
    "        temp_fullpath=os.path.join(directory_path, contentitem)\n",
    "        #temp_file=open(temp_fullpath,'r',encoding='utf-8',errors='ignore')\n",
    "        temp_file=open(temp_fullpath,'r')\n",
    "        temp_text=temp_file.read()\n",
    "        temp_file.close()\n",
    "        # Store the read filename and content\n",
    "        crawled_filenames.append(contentitem)\n",
    "        mycrawled_texts.append(temp_text)\n",
    "        num_files_read=num_files_read+1\n",
    "    return(crawled_filenames, mycrawled_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "negative-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "crawled_filenames, mycrawled_texts = basicfilecrawler('20news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lightweight-performer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['100521', '101551', '101552', '101553']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_filenames[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annual-horizon",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!ogicse!uwm.edu!wupost!uunet!brunix!cs.brown.edu!cs012055\\nFrom: cs012055@cs.brown.edu (Hok-Chung Tsang)\\nNewsgroups: rec.autos\\nSubject: Re: Saturn's Pricing Policy\\nMessage-ID: <1993Apr5.230808.581@cs.brown.edu>\\nDate: 5 Apr 93 23:08:08 GMT\\nArticle-I.D.: cs.1993Apr5.230808.581\\nReferences: <C4oxwp.KKM@news.cso.uiuc.edu> <C4vIr5.L3r@shuksan.ds.boeing.com>\\nSender: news@cs.brown.edu\\nOrganization: Brown Computer Science Dept.\\nLines: 51\\n\\nIn article <C4vIr5.L3r@shuksan.ds.boeing.com>, fredd@shuksan (Fred Dickey) writes:\\n|> CarolinaFan@uiuc (cka52397@uxa.cso.uiuc.edu) wrote:\\n|> : \\tI have been active in defending Saturn lately on the net and would\\n|> : like to state my full opinion on the subject, rather than just reply to others'\\n|> : points.\\n|> : \\t\\n|> : \\tThe biggest problem some people seem to be having is that Saturn\\n|> : Dealers make ~$2K on a car.  I think most will agree with me that the car is\\n|> : comparably priced with its competitors, that is, they aren't overpriced \\n|> : compared to most cars in their class.  I don't understand the point of \\n|> : arguing over whether the dealer makes the $2K or not?  \\n|> \\n|> I have never understood what the big deal over dealer profits is either.\\n|> The only thing that I can figure out is that people believe that if\\n|> they minimize the dealer profit they will minimize their total out-of-pocket\\n|> expenses for the car. While this may be true in some cases, I do not\\n|> believe that it is generally true. I bought a Saturn SL in January of '92.\\n|> AT THAT TIME, based on studying car prices, I decided that there was\\n|> no comparable car that was priced as cheaply as the Saturn. Sure, maybe I\\n|> could have talked the price for some other car to the Saturn price, but\\n|> my out-of-pocket expenses wouldn't have been any different. What's important\\n|> to me is how much money I have left after I buy the car. REDUCING DEALER PROFIT\\n|> IS NOT THE SAME THING AS SAVING MONEY! Show me how reducing dealer profit\\n|> saves me money, and I'll believe that it's important. My experience has\\n|> been that reducing dealer profit does not necessarily save me money.\\n|> \\n|> Fred\\n\\n\\nSay, you bought your Saturn at $13k, with a dealer profit of $2k.\\nIf the dealer profit is $1000, then you would only be paying $12k for\\nthe same car.  So isn't that saving money?\\n\\nMoreover, if Saturn really does reduce the dealer profit margin by $1000, \\nthen their cars will be even better deals.  Say, if the price of a Saturn was\\nalready $1000 below market average for the class of cars, then after they\\nreduce the dealer profit, it would be $2000 below market average.  It will:\\n\\n1) Attract even more people to buy Saturns because it would SAVE THEM MONEY.\\n \\n2) Force the competitors to lower their prices to survive.\\n\\nNow, not only will Saturn owners benefit from a lower dealer profit, even \\nthe buyers for other cars will pay less.\\n\\nIsn't that saving money?\\n\\n\\n\\n$0.02,\\ndoug.\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycrawled_texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude header lines from each message\n",
    "excludedlinemarkers=['Xref:','Path:','From:','Newsgroups:','Subject:','Summary:',\n",
    "                     'Keywords:','Message-ID:','Date:','Expires:','Followup-To:','Distribution:', \n",
    "                     'Organization:','Approved:','Supersedes:','Lines:','NNTP-Posting-Host:', \n",
    "                     'References:','Sender:','In-Reply-To:','Article-I.D.:','Reply-To:', \n",
    "                     'Nntp-Posting-Host:', 'Newsgroup', 'document_id']\n",
    "\n",
    "for k in range(len(mycrawled_texts)):\n",
    "    print(k)\n",
    "    templines = mycrawled_texts[k].splitlines()\n",
    "    remaininglines = []\n",
    "    for l in range(len(templines)):\n",
    "        line_should_be_excluded = 0\n",
    "        for m in range(len(excludedlinemarkers)):\n",
    "            if len(templines[l]) >= len(excludedlinemarkers[m]):\n",
    "                if excludedlinemarkers[m] == templines[l][0:len(excludedlinemarkers[m])]:\n",
    "                    line_should_be_excluded = 1\n",
    "                    break\n",
    "        if line_should_be_excluded == 0:\n",
    "            remaininglines.append(templines[l])\n",
    "    mycrawled_texts[k] = '\\n'.join(remaininglines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "falling-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_texts = []\n",
    "# remove leading and trailing whitespaces\n",
    "for k in range(len(mycrawled_texts)):\n",
    "    temp_texts = mycrawled_texts[k].strip()\n",
    "    temp_texts = ' '.join(temp_texts.split())\n",
    "    downloaded_texts.append(temp_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "according-coaching",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(downloaded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becoming-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize downloaded texts and change them to NLTK format\n",
    "mydownloaded_nltk_texts = []\n",
    "for k in range(len(downloaded_texts)):\n",
    "    temp_tokenizedtext = nltk.word_tokenize(downloaded_texts[k])\n",
    "    temp_nltktext = nltk.Text(temp_tokenizedtext)\n",
    "    mydownloaded_nltk_texts.append(temp_nltktext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "communist-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all downloaded texts lowercase\n",
    "mydownloaded_lowercase_texts = []\n",
    "for k in range(len(mydownloaded_nltk_texts)):\n",
    "    temp_lowercase_text = []\n",
    "    for l in range(len(mydownloaded_nltk_texts[k])):\n",
    "        lowercase_word = mydownloaded_nltk_texts[k][l].lower()\n",
    "        temp_lowercase_text.append(lowercase_word)\n",
    "    temp_lowercasetest = nltk.Text(temp_lowercase_text)\n",
    "    mydownloaded_lowercase_texts.append(temp_lowercase_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "concrete-colors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a POS tag for WordNet\n",
    "def tagtowordnet(postag):\n",
    "    wordnettag=-1\n",
    "    if postag[0]=='N':\n",
    "        wordnettag='n'\n",
    "    elif postag[0]=='V':\n",
    "        wordnettag='v'\n",
    "    elif postag[0]=='J':\n",
    "        wordnettag='a'\n",
    "    elif postag[0]=='R':\n",
    "        wordnettag='r'\n",
    "    return(wordnettag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "turkish-color",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tunde\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# POS tag and lemmatize the loaded texts\n",
    "# Download tagger and wordnet resources if you do not have them already\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# lemmatize downloaded texts\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizetext(nltktexttolemmatize):\n",
    "    # tag the text with POS tags\n",
    "    taggedtext = nltk.pos_tag(nltktexttolemmatize)\n",
    "    # lemmatize each word text\n",
    "    lemmatizedtext = []\n",
    "    for l in range(len(taggedtext)):\n",
    "        # Lemmatize a word using the WordNet converted POS tag\n",
    "        wordtolemmatize = taggedtext[l][0]\n",
    "        wordnettag = tagtowordnet(taggedtext[l][1])\n",
    "        if wordnettag != -1:\n",
    "            lemmatizedword = lemmatizer.lemmatize(wordtolemmatize, wordnettag)\n",
    "        else:\n",
    "            lemmatizedword = wordtolemmatize\n",
    "        # store the lemmatized word\n",
    "        lemmatizedtext.append(lemmatizedword)\n",
    "    return(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "automotive-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydownloaded_lemmatizedtexts = []\n",
    "for k in range(len(mydownloaded_lowercase_texts)):\n",
    "    lemmatizedtext = lemmatizetext(mydownloaded_lowercase_texts[k])\n",
    "    lemmatizedtext = nltk.Text(lemmatizedtext)\n",
    "    mydownloaded_lemmatizedtexts.append(lemmatizedtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "environmental-server",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the vocabularies of each document\n",
    "myvocabularies = []\n",
    "myindices_in_vocabularies = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # get unique words and where they occur\n",
    "    temptext = mydownloaded_lemmatizedtexts[k]\n",
    "    unique_results = np.unique(temptext, return_inverse = True)\n",
    "    unique_words = unique_results[0]\n",
    "    word_indices = unique_results[1]\n",
    "    # store the vocabularies and the indices\n",
    "    myvocabularies.append(unique_words)\n",
    "    myindices_in_vocabularies.append(word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bound-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unify the vocabularies\n",
    "# first concatenate all vocabularies\n",
    "tempvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    tempvocabulary.extend(myvocabularies[k])\n",
    "\n",
    "# find unique words among all the vocabularies\n",
    "uniqueresults = np.unique(tempvocabulary, return_inverse = True)\n",
    "unifiedvocabulary = uniqueresults[0]\n",
    "wordindices = uniqueresults[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "legendary-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate previous indices to the unified vocabulary\n",
    "# must keep track where each vocabulary started in the concatenated one\n",
    "vocabularystart = 0\n",
    "myindices_in_unifiedvocabulary = []\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    # in order to shift word indices, we must temporarily\n",
    "    # convert their data type to a numpy array\n",
    "    tempindices = np.array(myindices_in_vocabularies[k])\n",
    "    tempindices = tempindices + vocabularystart\n",
    "    tempindices = wordindices[tempindices]\n",
    "    myindices_in_unifiedvocabulary.append(tempindices)\n",
    "    vocabularystart = vocabularystart + len(myvocabularies[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bearing-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total count of each unique word over all the downloaded documents\n",
    "unifiedvocabulary_totaloccurrencecounts = np.zeros((len(unifiedvocabulary),1))\n",
    "\n",
    "# count occurrences\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    occurrencecounts = np.zeros((len(unifiedvocabulary), 1))\n",
    "    for l in range(len(myindices_in_unifiedvocabulary[k])):\n",
    "        occurrencecounts[myindices_in_unifiedvocabulary[k][l]] = (occurrencecounts[myindices_in_unifiedvocabulary[k][l]]\n",
    "                                                                 + 1)\n",
    "    unifiedvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts + occurrencecounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "european-geometry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>' 'the' ',' '.' '--' 'be' 'a' 'to' 'i' ')' 'in' 'and' '(' 'of' ':' '@'\n",
      " 'have' 'that' 'it' 'you' '!' 'do' 'for' '?' \"'s\" 'on' '|' \"n't\" '-'\n",
      " 'this' 'with' 'but' 'he' \"''\" 'not' '0' 'they' '...' '1' 'as' '<' '``'\n",
      " 'at' 'if' 'get' 'write' 'or' 'my' '2' 'article' 'game' 'go' 'would' '#'\n",
      " 'can' 'about' 'one' 'will' 'all' 'there' 'an' 'what' 'from' 'out' 'so'\n",
      " '*' 'good' 'car' 'like' 'year' 'by' 'think' 'me' 'team' 'up' 'his' '%'\n",
      " 'just' 'when' '3' 'make' 'more' 'no' 'your' 'know' 'any' 'say' 'who'\n",
      " 'than' 'we' '4' 'time' 'some' 'play' 'well' 'see' 'how' 'player' \"'m\"\n",
      " 'only']\n",
      "[40640. 40004. 39497. 39447. 35859. 27283. 17906. 17356. 16024. 14883.\n",
      " 14093. 14011. 13692. 13179. 11444.  9901.  9497.  9222.  8767.  7162.\n",
      "  7079.  7035.  7012.  6686.  6181.  6043.  5925.  5222.  4878.  4748.\n",
      "  4509.  4285.  4178.  4150.  4000.  3977.  3935.  3868.  3795.  3751.\n",
      "  3648.  3629.  3567.  3525.  3266.  3261.  3238.  3217.  2837.  2745.\n",
      "  2726.  2692.  2684.  2518.  2408.  2392.  2371.  2306.  2274.  2270.\n",
      "  2257.  2255.  2250.  2216.  2203.  2109.  2092.  2077.  2070.  2061.\n",
      "  2043.  2039.  1972.  1952.  1931.  1879.  1869.  1864.  1857.  1817.\n",
      "  1761.  1755.  1755.  1752.  1750.  1740.  1733.  1719.  1572.  1567.\n",
      "  1560.  1520.  1490.  1458.  1420.  1397.  1362.  1349.  1317.  1303.]\n"
     ]
    }
   ],
   "source": [
    "# top-100 words according to the largest total occurrence count\n",
    "highest_totaloccurrences_indices = np.argsort(-1*unifiedvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(unifiedvocabulary[highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(unifiedvocabulary_totaloccurrencecounts[highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "regulated-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Vocabulary pruning\n",
    "nltkstopwords = nltk.corpus.stopwords.words('english')\n",
    "pruningdecisions = np.zeros((len(unifiedvocabulary),1))\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    # Rule 1: check the nltk stop word list\n",
    "    if (unifiedvocabulary[k] in nltkstopwords):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 2: if the word is too short\n",
    "    if (len(unifiedvocabulary[k]) < 3):\n",
    "        pruningdecisions[k] = 1\n",
    "      # Rule 3: if the word is too long\n",
    "    if (len(unifiedvocabulary[k]) > 20):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 4: if the word is in the top 1% of frequent words\n",
    "    if (k in highest_totaloccurrences_indices[0:int(np.floor(len(unifiedvocabulary)*0.01))]):\n",
    "        pruningdecisions[k] = 1\n",
    "    # Rule 5: if the word occurs less than 4 times\n",
    "    if(unifiedvocabulary_totaloccurrencecounts[k] < 4):\n",
    "        pruningdecisions[k] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "allied-windows",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 words after pruning the unified vocabulary:\n",
      "\n",
      "['ago' 'cost' 'large' 'city' 'mind' 'total' 'mention' 'brave' 'man' 'ford'\n",
      " 'penalty' 'final' 'anyway' 'bill' 'coach' 'idea' 'saw' 'performance'\n",
      " 'beat' 'version' 'rule' 'montreal' 'hitter' 'seat' 'group' 'friend' 'ice'\n",
      " 'today' 'face' 'although' 'order' 'almost' 'comment' 'minute' 'stats'\n",
      " 'bos' 'hold' 'det' 'follow' 'job' 'american' 'insurance' 'month' 'smith'\n",
      " 'past' 'local' 'easy' 'honda' 'tie' 'cal' 'news' 'hell' 'x-newsreader'\n",
      " 'wait' 'ticket' 'bring' 'jet' 'walk' 'hope' 'helmet' 'rider' 'joe'\n",
      " 'stuff' 'morris' 'van' 'left' 'compare' 'note' 'word' 'york' 'experience'\n",
      " 'center' 'flyer' 'information' 'add' 'break' 'defense' 'puck' 'set' 'pen'\n",
      " 'rear' 'contact' 'netcom.com' 'others' 'e-mail' 'later' 'young' 'claim'\n",
      " 'late' 'whether' 'tor' 'design' 'matter' 'pit' 'life' 'pull' 'decide'\n",
      " 'instead' 'open' 'difference']\n",
      "[247. 246. 246. 246. 245. 244. 244. 243. 242. 241. 240. 240. 236. 232.\n",
      " 231. 230. 229. 229. 228. 225. 223. 223. 223. 222. 222. 222. 222. 221.\n",
      " 221. 220. 219. 219. 218. 217. 217. 217. 216. 215. 215. 213. 213. 213.\n",
      " 212. 211. 211. 211. 210. 210. 210. 209. 208. 208. 207. 206. 206. 206.\n",
      " 204. 204. 204. 204. 203. 203. 203. 202. 201. 201. 201. 200. 200. 200.\n",
      " 199. 198. 197. 197. 197. 196. 196. 196. 196. 196. 196. 196. 195. 195.\n",
      " 193. 193. 192. 192. 191. 189. 189. 189. 189. 187. 187. 187. 186. 186.\n",
      " 186. 185.]\n"
     ]
    }
   ],
   "source": [
    "print('Top-100 words after pruning the unified vocabulary:\\n')\n",
    "remaining_indices = np.squeeze(np.where(pruningdecisions==0)[0])\n",
    "remaining_vocabulary = unifiedvocabulary[remaining_indices]\n",
    "remainingvocabulary_totaloccurrencecounts = unifiedvocabulary_totaloccurrencecounts[remaining_indices]\n",
    "remaining_highest_totaloccurrences_indices = np.argsort(-1*remainingvocabulary_totaloccurrencecounts, axis=0)\n",
    "print(np.squeeze(remaining_vocabulary[remaining_highest_totaloccurrences_indices[0:100]]))\n",
    "print(np.squeeze(remainingvocabulary_totaloccurrencecounts[remaining_highest_totaloccurrences_indices[0:100]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "forty-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Get indices of documents to remaining words\n",
    "oldtopruned=[]\n",
    "tempind=-1\n",
    "for k in range(len(unifiedvocabulary)):\n",
    "    if pruningdecisions[k]==0:\n",
    "        tempind=tempind+1\n",
    "        oldtopruned.append(tempind)\n",
    "    else:\n",
    "        oldtopruned.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sapphire-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create pruned texts\n",
    "mycrawled_prunedtexts=[]\n",
    "myindices_in_prunedvocabulary=[]\n",
    "for k in range(len(mydownloaded_lemmatizedtexts)):\n",
    "    #print(k)\n",
    "    temp_newindices=[]\n",
    "    temp_newdoc=[]\n",
    "    for l in range(len(mydownloaded_lemmatizedtexts[k])):\n",
    "        temp_oldindex=myindices_in_unifiedvocabulary[k][l]\n",
    "        temp_newindex=oldtopruned[temp_oldindex]\n",
    "        if temp_newindex!=-1:\n",
    "            temp_newindices.append(temp_newindex)\n",
    "            temp_newdoc.append(unifiedvocabulary[temp_oldindex])\n",
    "    mycrawled_prunedtexts.append(temp_newdoc)\n",
    "    myindices_in_prunedvocabulary.append(temp_newindices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hydraulic-component",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fred',\n",
       " 'dickey',\n",
       " 'uiuc',\n",
       " 'cka52397',\n",
       " 'uxa.cso.uiuc.edu',\n",
       " 'active',\n",
       " 'defend',\n",
       " 'saturn',\n",
       " 'lately',\n",
       " 'full',\n",
       " 'subject',\n",
       " 'reply',\n",
       " 'others',\n",
       " 'saturn',\n",
       " 'competitor',\n",
       " 'overprice',\n",
       " 'compare',\n",
       " 'class',\n",
       " 'understand',\n",
       " 'whether',\n",
       " 'understand',\n",
       " 'profit',\n",
       " 'figure',\n",
       " 'minimize',\n",
       " 'profit',\n",
       " 'minimize',\n",
       " 'total',\n",
       " 'out-of-pocket',\n",
       " 'expense',\n",
       " 'generally',\n",
       " 'saturn',\n",
       " 'january',\n",
       " \"'92\",\n",
       " 'study',\n",
       " 'decide',\n",
       " 'comparable',\n",
       " 'cheaply',\n",
       " 'saturn',\n",
       " 'saturn',\n",
       " 'out-of-pocket',\n",
       " 'expense',\n",
       " 'important',\n",
       " 'reduce',\n",
       " 'profit',\n",
       " 'reduce',\n",
       " 'profit',\n",
       " 'important',\n",
       " 'experience',\n",
       " 'reduce',\n",
       " 'profit',\n",
       " 'necessarily',\n",
       " 'fred',\n",
       " 'saturn',\n",
       " '13k',\n",
       " 'profit',\n",
       " 'profit',\n",
       " '1000',\n",
       " '12k',\n",
       " 'moreover',\n",
       " 'saturn',\n",
       " 'reduce',\n",
       " 'profit',\n",
       " 'margin',\n",
       " '1000',\n",
       " 'saturn',\n",
       " 'already',\n",
       " '1000',\n",
       " 'market',\n",
       " 'class',\n",
       " 'reduce',\n",
       " 'profit',\n",
       " '2000',\n",
       " 'market',\n",
       " 'attract',\n",
       " 'saturn',\n",
       " 'force',\n",
       " 'competitor',\n",
       " 'lower',\n",
       " 'survive',\n",
       " 'saturn',\n",
       " 'benefit',\n",
       " 'profit',\n",
       " 'buyer',\n",
       " '0.02',\n",
       " 'doug']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mycrawled_prunedtexts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "informal-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_tagged_docs = []\n",
    "for k in range(len(mycrawled_prunedtexts)):\n",
    "    doctag = 'doc' + str(k)\n",
    "    tagged_document = gensim.models.doc2vec.TaggedDocument(mycrawled_prunedtexts[k], [doctag])\n",
    "    gensim_tagged_docs.append(tagged_document)\n",
    "# Create a dictionary from the documents\n",
    "gensim_dictionary = gensim.corpora.Dictionary(mycrawled_prunedtexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "quality-spell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gensim_tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "grand-variable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['fred', 'dickey', 'uiuc', 'cka52397', 'uxa.cso.uiuc.edu', 'active', 'defend', 'saturn', 'lately', 'full', 'subject', 'reply', 'others', 'saturn', 'competitor', 'overprice', 'compare', 'class', 'understand', 'whether', 'understand', 'profit', 'figure', 'minimize', 'profit', 'minimize', 'total', 'out-of-pocket', 'expense', 'generally', 'saturn', 'january', \"'92\", 'study', 'decide', 'comparable', 'cheaply', 'saturn', 'saturn', 'out-of-pocket', 'expense', 'important', 'reduce', 'profit', 'reduce', 'profit', 'important', 'experience', 'reduce', 'profit', 'necessarily', 'fred', 'saturn', '13k', 'profit', 'profit', '1000', '12k', 'moreover', 'saturn', 'reduce', 'profit', 'margin', '1000', 'saturn', 'already', '1000', 'market', 'class', 'reduce', 'profit', '2000', 'market', 'attract', 'saturn', 'force', 'competitor', 'lower', 'survive', 'saturn', 'benefit', 'profit', 'buyer', '0.02', 'doug'], tags=['doc1'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gensim_tagged_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "floating-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vecmodel = gensim.models.doc2vec.Doc2Vec(gensim_tagged_docs, vector_size=10, window=5, min_count=1, \n",
    "                                             workers=4, dm_concat=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "endangered-belarus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32201952, -0.07698996, -0.31582958,  0.10277903, -0.07306157,\n",
       "       -0.16128775,  0.04651752,  0.6257165 , -0.06180112, -0.05683586],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vecmodel['doc1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "alike-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_names = ['101551', '103118 (2)', '98657', '52550'] # 103118 has a duplicate. Only 103118 (2) is part of\n",
    "                                                       # part of rec.motorcycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "administrative-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_indices = []\n",
    "for doc_name in doc_names:\n",
    "    doc_indices.append(crawled_filenames.index(doc_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "confidential-destruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 681, 3998, 2997]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indices # corresponding document indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "conditional-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances between doc 101551 and other documents\n",
    "dist_101551_103118 = euclidean(doc2vecmodel['doc1'], doc2vecmodel['doc681'])\n",
    "dist_101551_98657 = euclidean(doc2vecmodel['doc1'], doc2vecmodel['doc3998'])\n",
    "dist_101551_52550 = euclidean(doc2vecmodel['doc1'], doc2vecmodel['doc2997'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "worthy-cassette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8489734530448914, 0.9890953302383423, 0.8680312037467957)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_101551_103118, dist_101551_98657, dist_101551_52550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "preliminary-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances between 103118 and (98657 and 52550)\n",
    "dist_103118_98657 = euclidean(doc2vecmodel['doc681'], doc2vecmodel['doc3998'])\n",
    "dist_103118_52550 = euclidean(doc2vecmodel['doc681'], doc2vecmodel['doc2997'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "after-grill",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.2827506065368652, 1.0138620138168335)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_103118_98657, dist_103118_52550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "desirable-lightweight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance between 98657 and 52550\n",
    "dist_98657_52550 = euclidean(doc2vecmodel['doc3998'], doc2vecmodel['doc2997'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "angry-enemy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8446446061134338"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_98657_52550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cooperative-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.32201952, -0.07698996, -0.31582958,  0.10277903, -0.07306157,\n",
       "        -0.16128775,  0.04651752,  0.6257165 , -0.06180112, -0.05683586],\n",
       "       dtype=float32),\n",
       " array([-0.1675683 , -0.01746913,  0.10660884, -0.20813224,  0.04478887,\n",
       "        -0.04763159, -0.01295135,  0.30668226, -0.13105063,  0.19904618],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vecmodel['doc1'], doc2vecmodel['doc681']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "amber-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.61556876, -0.15115848,  0.19895129,  0.32541257,  0.51497835,\n",
       "        -0.11751217,  0.38376427,  0.5241279 , -0.21683079, -0.33301568],\n",
       "       dtype=float32),\n",
       " array([ 0.19670025, -0.19116794, -0.2614279 ,  0.04757068,  0.40910816,\n",
       "        -0.14905551,  0.26861793,  0.3582254 , -0.64323956, -0.22158138],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vecmodel['doc3998'], doc2vecmodel['doc2997']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-testing",
   "metadata": {},
   "source": [
    "**The closest documents are from the same newsgroup**\n",
    "- **Documents 101551 and 103118 are the closest when either of them is compared with other documents**\n",
    "- **Documents 98657 and 52550 are the closest when either of them is compared with other documents**\n",
    "- **The contents of the closest pair of documents seem to match that of the original documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-lunch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-folks",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
